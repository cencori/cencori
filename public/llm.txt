# Cencori - LLM.txt Documentation
> One API for every AI provider with built-in security, observability, and cost control.
> Use this document to quickly integrate Cencori into your project.

## What is Cencori?

Cencori is the unified AI infrastructure for production applications. Instead of integrating with OpenAI, Anthropic, and Google separately, you integrate once with Cencori and get:
- **One SDK** for all providers (OpenAI, Anthropic, Google)
- **Automatic security** - PII detection, prompt injection protection, content filtering
- **Complete observability** - audit logs, analytics, cost tracking
- **Multi-provider flexibility** - switch models with one parameter

## Quick Start

### 1. Install the SDK

**JavaScript/TypeScript:**
```bash
npm install cencori
# or: yarn add cencori
# or: pnpm add cencori
```

**Python:**
```bash
pip install cencori
```

**Go:**
```bash
go get github.com/cencori/cencori-go
```

### 2. Set Environment Variables

```bash
# .env
CENCORI_API_KEY=csk_your_secret_key_here  # Server-side (csk_ prefix)
```

### 3. Initialize the Client

```typescript
// lib/cencori.ts
import { Cencori } from "cencori";

export const cencori = new Cencori({
  apiKey: process.env.CENCORI_API_KEY!,
});
```

### 4. Make Your First Request

```typescript
// app/api/chat/route.ts
import { cencori } from "@/lib/cencori";

export async function POST(req: Request) {
  const { messages } = await req.json();

  const response = await cencori.ai.chat({
    model: "gpt-4o", // or "claude-3-opus", "gemini-2.5-flash"
    messages: messages,
    temperature: 0.7,
    maxTokens: 1000,
  });

  return Response.json({
    content: response.content,
    usage: response.usage,
  });
}
```

**Go:**
```go
// main.go
package main

import (
    "context"
    "fmt"
    "os"
    "github.com/cencori/cencori-go"
)

func main() {
    client, _ := cencori.NewClient(
        cencori.WithAPIKey(os.Getenv("CENCORI_API_KEY")),
    )

    resp, _ := client.Chat.Create(context.Background(), &cencori.ChatParams{
        Model: "gpt-4o",
        Messages: []cencori.Message{
            {Role: "user", Content: "Hello!"},
        },
    })

    fmt.Println(resp.Choices[0].Message.Content)
}
```

## Vercel AI SDK Integration

Using Vercel AI SDK? Install `@cencori/ai-sdk` for drop-in compatibility:

```bash
npm install @cencori/ai-sdk ai
```

```typescript
// app/api/chat/route.ts
import { cencori } from "@cencori/ai-sdk";
import { streamText } from "ai";

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = await streamText({
    model: cencori("gemini-2.5-flash"),
    messages,
  });

  return result.toUIMessageStreamResponse();
}
```

Works with `streamText()`, `generateText()`, `useChat()`, and all Vercel AI SDK features.

## TanStack AI Integration

Using TanStack AI? Install `@cencori/ai-sdk` with the TanStack subpath:

```bash
npm install @cencori/ai-sdk @tanstack/ai
```

```typescript
import { cencori } from "@cencori/ai-sdk/tanstack";

const adapter = cencori("gpt-4o");

// Streaming
for await (const chunk of adapter.chatStream({
  messages: [{ role: "user", content: "Hello!" }]
})) {
  if (chunk.type === "content") {
    console.log(chunk.delta);
  }
}
```

Works with `chatStream()`, structured output, and all TanStack AI features.

## Tool Calling / Function Calling

Cencori supports tool calling across all providers with a unified interface.

### Vercel AI SDK Tool Calling

```typescript
import { cencori } from "@cencori/ai-sdk";
import { generateText, tool } from "ai";
import { z } from "zod";

const result = await generateText({
  model: cencori("gpt-4o"),
  prompt: "What is the weather in San Francisco?",
  tools: {
    getWeather: tool({
      description: "Get the current weather for a location",
      parameters: z.object({
        location: z.string().describe("The city name"),
      }),
      execute: async ({ location }) => {
        return { temperature: 72, condition: "sunny" };
      },
    }),
  },
});

// Access tool calls
for (const toolCall of result.toolCalls) {
  console.log(`Called ${toolCall.toolName} with args:`, toolCall.args);
}
```

### TanStack AI Tool Calling

```typescript
import { cencori } from "@cencori/ai-sdk/tanstack";

const adapter = cencori("gpt-4o");

for await (const chunk of adapter.chatStream({
  messages: [{ role: "user", content: "Get weather for NYC" }],
  tools: {
    getWeather: {
      name: "getWeather",
      description: "Get weather for a location",
      inputSchema: { type: "object", properties: { location: { type: "string" } } },
    },
  },
})) {
  if (chunk.type === "tool_call") {
    console.log("Tool call:", chunk.toolCall);
    // { id: "call_123", type: "function", function: { name: "getWeather", arguments: '{"location":"NYC"}' } }
  }
}

## Chat API Reference

### Basic Usage

```typescript
const response = await cencori.ai.chat({
  model: "gpt-4o",
  messages: [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Hello!" }
  ],
  temperature: 0.7,
  maxTokens: 1000,
});

console.log(response.content);
```

**Python:**
```python
from cencori import Cencori

cencori = Cencori(api_key="your-api-key")

response = cencori.ai.chat(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    model="gpt-4o",
    temperature=0.7,
    max_tokens=1000,
)

print(response.content)
```

**Go:**
```go
resp, _ := client.Chat.Create(ctx, &cencori.ChatParams{
    Model: "gpt-4o",
    Messages: []cencori.Message{
        {Role: "system", Content: "You are a helpful assistant."},
        {Role: "user", Content: "Hello!"},
    },
    Temperature: cencori.Float64(0.7),
    MaxTokens:   cencori.Int(1000),
})
fmt.Println(resp.Choices[0].Message.Content)
```

### Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `model` | string | No | AI model (default: gemini-2.5-flash) |
| `messages` | array | Yes | Conversation history with role and content |
| `temperature` | number | No | Randomness (0-1) |
| `maxTokens` | number | No | Max response tokens |
| `userId` | string | No | User ID for rate limiting |

### Response Format

```typescript
{
  content: "Hello! How can I help?",
  model: "gpt-4o",
  provider: "openai",
  usage: {
    prompt_tokens: 13,
    completion_tokens: 7,
    total_tokens: 20
  },
  cost_usd: 0.0003,
  finish_reason: "stop"
}
```

## Streaming

Stream responses for better UX using `chatStream()`:

```typescript
// Server route
import { Cencori } from "cencori";

export async function POST(req: Request) {
  const { messages } = await req.json();
  
  const cencori = new Cencori({
    apiKey: process.env.CENCORI_API_KEY!,
  });

  const stream = cencori.ai.chatStream({
    model: "gpt-4o",
    messages,
  });

  const encoder = new TextEncoder();
  const readableStream = new ReadableStream({
    async start(controller) {
      for await (const chunk of stream) {
        // chunk.delta contains the streamed text
        controller.enqueue(encoder.encode(chunk.delta));
      }
      controller.close();
    },
  });

  return new Response(readableStream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
    },
  });
}
```

### Stream Chunk Format

```typescript
{
  delta: "Hello",           // The text chunk
  finish_reason?: "stop",   // Only on last chunk: 'stop' | 'length' | 'content_filter' | 'error'
  error?: "Rate limit exceeded"  // Error message if stream failed
}
```

### Handling Stream Errors

```typescript
for await (const chunk of stream) {
  // Check for errors in stream
  if (chunk.error) {
    console.error('Stream error:', chunk.error);
    break;
  }
  controller.enqueue(encoder.encode(chunk.delta));
}
```

Client-side streaming:

```typescript
const res = await fetch('/api/chat', {
  method: 'POST',
  body: JSON.stringify({ messages }),
});

const reader = res.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  const chunk = decoder.decode(value);
  setResponse(prev => prev + chunk);
}
```

**Go (Client, Streaming):**
```go
stream, _ := client.Chat.Stream(context.Background(), &cencori.ChatParams{
    Model: "claude-3-opus",
    Messages: []cencori.Message{
        {Role: "user", Content: "Tell me a story"},
    },
})

for chunk := range stream {
    if len(chunk.Choices) > 0 {
        fmt.Print(chunk.Choices[0].Delta.Content)
    }
}
```

## Supported Models (14+ Providers)

### OpenAI
| Model | Context | Description |
|-------|---------|-------------|
| `gpt-5` | 256K | Latest flagship model |
| `gpt-4o` | 128K | Omni-modal model |
| `gpt-4o-mini` | 128K | Fast and cost-effective |
| `o3` | 200K | Most advanced reasoning |
| `o1` | 128K | Advanced reasoning model |

### Anthropic
| Model | Context | Description |
|-------|---------|-------------|
| `claude-opus-4` | 200K | Most capable Claude |
| `claude-sonnet-4` | 200K | Balanced speed & capability |
| `claude-3-5-sonnet` | 200K | Balance of speed and capability |
| `claude-3-5-haiku` | 200K | Fast and efficient |

### Google Gemini
| Model | Context | Description |
|-------|---------|-------------|
| `gemini-3-pro` | 2M | Most powerful Gemini |
| `gemini-2.5-flash` | 1M | Thinking capabilities |
| `gemini-2.0-flash` | 1M | Fast model |

### xAI (Grok)
| Model | Context | Description |
|-------|---------|-------------|
| `grok-4` | 256K | Enhanced reasoning |
| `grok-4.1` | 256K | Improved multimodal |
| `grok-3` | 128K | DeepSearch, Big Brain Mode |

### Additional Providers
- **Mistral**: mistral-large-3, codestral, devstral
- **DeepSeek**: deepseek-v3.2, deepseek-reasoner
- **Meta (Llama)**: llama-4-maverick, llama-3.3-70b
- **Groq**: Fast inference for Llama models
- **Cohere**: command-a, command-r+
- **Perplexity**: sonar-pro, sonar-reasoning-pro
- **Together AI**: Access to 100+ open models
- **OpenRouter**: Meta-provider for all models

### Model Selection Guide
- **Chat/General**: `gemini-2.5-flash` (fast, cheap, good quality)
- **Code**: `gpt-4o` or `codestral` (best coding)
- **Reasoning**: `o3` or `deepseek-reasoner`
- **Documents**: `claude-opus-4` (200K context, strong reasoning)
- **Search**: `sonar-pro` (web-connected)

## Multi-Provider Switching

Switch providers with one parameter:

```typescript
// OpenAI
await cencori.ai.chat({ model: "gpt-4o", messages });

// Anthropic
await cencori.ai.chat({ model: "claude-3-opus", messages });

// Google
await cencori.ai.chat({ model: "gemini-2.5-flash", messages });
```

All responses have the same format regardless of provider!

## Image Generation

Generate images from text prompts using multiple AI providers with a unified API.

### Supported Models

| Provider | Model | Description |
|----------|-------|-------------|
| **OpenAI** | `gpt-image-1.5` | Best text rendering, top ELO rating |
| **OpenAI** | `gpt-image-1` | ChatGPT image generation |
| **OpenAI** | `dall-e-3` | High quality, creative |
| **OpenAI** | `dall-e-2` | Fast, cost-effective |
| **Google** | `gemini-3-pro-image` | High photorealism, fast |
| **Google** | `imagen-3` | Google Imagen 3 |

### Basic Usage

```typescript
const response = await cencori.ai.generateImage({
  prompt: "A futuristic city at sunset with flying cars",
  model: "gpt-image-1.5",  // or "dall-e-3", "gemini-3-pro-image"
  size: "1024x1024",
  quality: "hd"
});

console.log(response.images[0].url);  // URL to the generated image
```

### Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `prompt` | string | Yes | Text description of the image |
| `model` | string | No | Image model (default: dall-e-3) |
| `size` | string | No | Image size (1024x1024, 1792x1024, etc.) |
| `n` | number | No | Number of images (1-4, varies by model) |
| `quality` | string | No | 'standard' or 'hd' (DALL-E 3/GPT Image only) |
| `style` | string | No | 'vivid' or 'natural' (DALL-E 3 only) |
| `responseFormat` | string | No | 'url' or 'b64_json' |

### Response Format

```typescript
{
  images: [
    {
      url: "https://...",        // Image URL (if responseFormat='url')
      b64Json: "...",            // Base64 data (if responseFormat='b64_json')
      revisedPrompt: "..."       // Model's revised prompt
    }
  ],
  model: "gpt-image-1.5",
  provider: "openai"
}
```

## Embeddings

Generate vector embeddings for text using multiple providers.

### Supported Models

| Provider | Model | Dimensions | Description |
|----------|-------|------------|-------------|
| **OpenAI** | `text-embedding-3-large` | 3072 | Best quality |
| **OpenAI** | `text-embedding-3-small` | 1536 | Fast and efficient |
| **OpenAI** | `text-embedding-ada-002` | 1536 | Legacy model |
| **Google** | `text-embedding-004` | 768 | Google embedding model |
| **Cohere** | `embed-english-v3.0` | 1024 | English optimized |
| **Cohere** | `embed-multilingual-v3.0` | 1024 | Multilingual support |

### Usage

```typescript
const response = await cencori.ai.embeddings({
  model: 'text-embedding-3-small',
  input: 'Hello world'
});

console.log(response.embeddings[0]); // [0.1, 0.2, ...]
```

### API Request

```typescript
POST /api/ai/embeddings
Headers: { "CENCORI_API_KEY": "csk_..." }
Body: {
  "model": "text-embedding-3-small",
  "input": "Hello world",
  "dimensions": 1536  // optional, for variable dimension models
}
```

## Audio

Cencori supports audio transcription and text-to-speech.

### Transcriptions

Convert audio to text using Whisper models.

```typescript
POST /api/ai/audio/transcriptions
Headers: { "CENCORI_API_KEY": "csk_..." }
Body: FormData with 'file' (audio file) and optional 'model', 'language', 'prompt'
```

### Text-to-Speech

Generate audio from text.

```typescript
POST /api/ai/audio/speech
Headers: { "CENCORI_API_KEY": "csk_..." }
Body: {
  "input": "Hello, welcome to Cencori!",
  "model": "tts-1",  // or "tts-1-hd"
  "voice": "alloy"   // alloy, echo, fable, onyx, nova, shimmer
}
```

## Moderation

Detect harmful content in text before it reaches your application.

### Supported Models

| Model | Description |
|-------|-------------|
| `text-moderation-latest` | Latest moderation model |
| `text-moderation-stable` | Stable moderation model |
| `omni-moderation-latest` | Multimodal (text + images) |

### Usage

```typescript
POST /api/ai/moderation
Headers: { "CENCORI_API_KEY": "csk_..." }
Body: {
  "input": "Text to check for harmful content",
  "model": "text-moderation-latest"
}
```

### Categories Detected

- hate, hate/threatening
- harassment, harassment/threatening
- self-harm, self-harm/intent, self-harm/instructions
- sexual, sexual/minors
- violence, violence/graphic

### Response

```typescript
{
  "id": "modr-...",
  "model": "text-moderation-latest",
  "results": [{
    "flagged": false,
    "categories": { "hate": false, "violence": false, ... },
    "category_scores": { "hate": 0.001, "violence": 0.002, ... }
  }]
}
```

## AI Memory (Context Store)

Vector storage for RAG, conversation history, and semantic search. Store and retrieve memories with automatic embedding generation.

### Quick Start

```typescript
import { Cencori } from 'cencori';

const cencori = new Cencori({ apiKey: 'csk_...' });

// Store a memory
await cencori.memory.store({
  namespace: 'company-docs',
  content: 'Our refund policy allows returns within 30 days',
  metadata: { category: 'policy', source: 'handbook' }
});

// Semantic search
const results = await cencori.memory.search({
  namespace: 'company-docs',
  query: 'what is our return policy?',
  limit: 5
});

console.log(results.results[0].content);
// "Our refund policy allows returns within 30 days"
```

### SDK Methods

| Method | Description |
|--------|-------------|
| `memory.store(options)` | Store a memory with auto-embedding |
| `memory.search(options)` | Semantic search across memories |
| `memory.get(id)` | Get memory by ID |
| `memory.delete(id)` | Delete a memory |
| `memory.storeBatch(namespace, items)` | Store multiple memories |
| `memory.createNamespace(options)` | Create a namespace |
| `memory.listNamespaces()` | List all namespaces |

### Store Memory

```typescript
await cencori.memory.store({
  namespace: 'conversations',
  content: 'User asked about pricing plans',
  metadata: {
    userId: 'user_123',
    sessionId: 'sess_456',
    timestamp: new Date().toISOString()
  },
  expiresAt: '2024-12-31T23:59:59Z' // optional TTL
});
```

### Search Memories

```typescript
const results = await cencori.memory.search({
  namespace: 'conversations',
  query: 'what did we discuss about pricing?',
  limit: 5,           // max results
  threshold: 0.7,     // similarity threshold (0-1)
  filter: {           // metadata filters
    userId: 'user_123'
  }
});

// Response format
{
  results: [
    {
      id: 'mem_...',
      content: 'User asked about pricing plans',
      metadata: { userId: 'user_123', ... },
      similarity: 0.92,
      createdAt: '2024-01-15T...'
    }
  ],
  query: 'what did we discuss about pricing?',
  namespace: 'conversations',
  count: 1,
  latencyMs: 45
}
```

### RAG (Retrieval-Augmented Generation)

Combine memory search with AI chat for context-aware responses:

```typescript
// Method 1: Use the RAG helper
const response = await cencori.ai.rag({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: 'What is our refund policy?' }],
  namespace: 'company-docs',
  limit: 5
});

console.log(response.message.content);
console.log(response.sources); // retrieved memories

// Method 2: Manual RAG
const memories = await cencori.memory.search({
  namespace: 'company-docs',
  query: 'refund policy'
});

const context = memories.results.map(m => m.content).join('\n\n');

const chat = await cencori.ai.chat({
  model: 'gpt-4o',
  messages: [
    { role: 'system', content: `Use this context:\n${context}` },
    { role: 'user', content: 'What is our refund policy?' }
  ]
});
```

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/memory/namespaces` | GET | List namespaces |
| `/api/memory/namespaces` | POST | Create namespace |
| `/api/memory/store` | POST | Store memory |
| `/api/memory/search` | POST | Search memories |
| `/api/memory/{id}` | GET | Get memory |
| `/api/memory/{id}` | DELETE | Delete memory |

### Dashboard

Manage memories visually at `/dashboard/[org]/[project]/memory`:
- Create and manage namespaces
- Browse memories with pagination
- Test semantic search
- View memory metadata and similarity scores

## Agent Frameworks

Cencori works with any OpenAI-compatible agent framework. Just point your `base_url` to Cencori:

**Prerequisite:** Add your provider API keys (OpenAI, Anthropic, Google) in your Cencori project settings first. Cencori routes requests to providers using your keys.

### CrewAI

```bash
export OPENAI_API_KEY=your_cencori_api_key
export OPENAI_API_BASE=https://api.cencori.com/v1
```

```python
from crewai import Agent, Task, Crew

researcher = Agent(role='Research Analyst', goal='Find insights')
crew = Crew(agents=[researcher], tasks=[...])
result = crew.kickoff()
# Every LLM call logged in Cencori!
```

### AutoGen

```python
config_list = [{
    "model": "gpt-4o",
    "api_key": "your_cencori_api_key",
    "base_url": "https://api.cencori.com/v1"
}]

assistant = AssistantAgent(name="assistant", llm_config={"config_list": config_list})
```

### LangChain

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o",
    api_key="your_cencori_api_key",
    base_url="https://api.cencori.com/v1"
)
```

### OpenAI SDK (Direct)

```python
from openai import OpenAI

client = OpenAI(
    api_key="your_cencori_api_key",
    base_url="https://api.cencori.com/v1"
)
```

**Benefits:** Full observability, automatic failover, rate limiting, security scanning for all agent calls.

## Provider Failover

Cencori automatically handles provider failures:

### How It Works
1. Request fails → Retries with exponential backoff (up to 3 times)
2. Still failing → Routes to fallback provider (e.g., OpenAI → Anthropic)
3. Model mapping → GPT-5 → Claude Opus 4 (automatic equivalent)

### Circuit Breaker
After 5 consecutive failures, provider is skipped entirely for 60 seconds.

### Fallback Response
When fallback is used, response includes:

```typescript
{
  content: "Hello!",
  model: "claude-opus-4",       // Actual model used
  provider: "anthropic",        // Actual provider
  fallback_used: true,          // Indicates fallback was triggered
  original_model: "gpt-5",      // What you requested
  original_provider: "openai"   // Original provider
}
```

### Configuration
Configure in Settings → Infrastructure:
- **Enable fallback**: On/Off
- **Fallback provider**: Preferred backup (e.g., Anthropic)
- **Max retries**: Attempts before failover (default: 3)

## Error Handling

```typescript
import { Cencori, SafetyError, RateLimitError, AuthenticationError } from "cencori";

try {
  const response = await cencori.ai.chat({
    model: "gpt-4o",
    messages,
  });
  return response;
} catch (error) {
  if (error instanceof SafetyError) {
    // Request blocked (prompt injection, PII, harmful content)
    return { error: "Content blocked", reasons: error.reasons };
  }
  if (error instanceof RateLimitError) {
    return { error: "Too many requests" };
  }
  if (error instanceof AuthenticationError) {
    return { error: "Invalid API key" };
  }
  throw error;
}
```

## Security Features

Cencori automatically protects against:

### Prompt Injection
Detects and blocks malicious inputs like:
- "Ignore all previous instructions..."
- "What is your system prompt?"
- Jailbreak attempts

### PII Detection
Scans for and filters:
- Social Security Numbers
- Credit card numbers
- Email addresses
- Phone numbers

### Content Filtering
Blocks harmful content before it reaches AI providers.

All security violations throw `SafetyError` with `reasons` array.

## Best Practices

1. **Set maxTokens** - Prevent unexpectedly long responses
2. **Include userId** - Enable per-user rate limiting
3. **Handle errors gracefully** - Use typed error classes
4. **Use chatStream() for chat UIs** - Better user experience
5. **Monitor token usage** - Check dashboard for costs
6. **Store API keys in env vars** - Never commit to version control
7. **Use system role for instructions** - Never concatenate with user input
8. **Use secret keys for servers** - Never expose `csk_` keys in client code
9. **Use publishable keys for browsers** - Use `cpk_` keys with domain whitelisting

## Cencori Scan (@cencori/scan)

Security scanner for AI apps. Detect hardcoded secrets, PII leaks, exposed routes, and security vulnerabilities — with AI-powered auto-fix.

### Quick Start

```bash
npx @cencori/scan
```

Run in any project directory to instantly scan for security issues.

### What It Detects

| Category | Examples |
|----------|----------|
| **API Keys & Secrets** | OpenAI, Anthropic, Google AI, Supabase, Stripe, AWS, GitHub, Firebase + 20 more |
| **PII** | Email addresses, phone numbers, SSNs, credit cards |
| **Exposed Routes** | Next.js/Express routes without authentication |
| **Vulnerabilities** | SQL injection, XSS, insecure CORS, hardcoded passwords |

### CLI Options

```bash
npx @cencori/scan            # Scan current directory
npx @cencori/scan ./path     # Scan specific path
npx @cencori/scan --json     # Output JSON (for CI/CD)
npx @cencori/scan --quiet    # Score only
npx @cencori/scan --no-prompt # Non-interactive mode
```

### AI Auto-Fix (Pro)

After scanning, enter `y` when prompted to auto-fix issues:

```
? Would you like Cencori to auto-fix these issues? (y/n)
> y

? Enter your Cencori API key: ************************
  (Get one at https://cencori.com/dashboard)

✔ API key saved to ~/.cencorirc
✔ Analyzing with AI...
✔ Applied 10 fixes
```

The AI will:
1. Analyze each issue for false positives
2. Generate secure code fixes
3. Apply fixes automatically

Your API key is saved for future scans.

### Programmatic Usage

```typescript
import { scan } from '@cencori/scan';

const result = await scan('./my-project');

console.log(result.score);        // 'A' | 'B' | 'C' | 'D' | 'F'
console.log(result.issues);       // Array of detected issues
console.log(result.filesScanned); // Number of files scanned
```

### Security Score

| Score | Meaning |
|-------|---------|
| **A-Tier** | Excellent - No issues |
| **B-Tier** | Good - Minor improvements |
| **C-Tier** | Fair - Some concerns |
| **D-Tier** | Poor - Significant issues |
| **F-Tier** | Critical - Secrets exposed |

### CI/CD Integration (GitHub Actions)

```yaml
name: Security Scan
on: [push, pull_request]
jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run Cencori Scan
        run: npx @cencori/scan --json > scan-results.json
      - name: Check for failures
        run: |
          SCORE=$(jq -r '.score' scan-results.json)
          if [[ "$SCORE" == "F" ]]; then
            echo "Security scan failed"
            exit 1
          fi
```

## API Key Types

Cencori supports two types of API keys:

### Secret Keys (`csk_`)
- For **server-side use only**
- Full access to all features
- Never expose in browser or client code
- Example: `csk_abc123...`

### Publishable Keys (`cpk_`)
- Safe for **browser/client-side use**
- Requires domain whitelisting
- Only works from allowed domains
- Example: `cpk_xyz789...`

### Legacy Keys (`cen_`)
- Existing keys created before key types
- Treated as secret keys
- Still fully functional

## Dashboard

After integration, visit your Cencori dashboard to see:
- Request logs with full prompts and responses
- Security incidents (blocked attacks)
- Usage analytics and costs
- Rate limit status

## Links

- Dashboard: https://cencori.com
- Full Docs: https://cencori.com/docs
- Pricing: https://cencori.com/pricing
- Cencori Scan (CLI): https://www.npmjs.com/package/@cencori/scan
- Cencori Scan (Web): https://scan.cencori.com