---
title: "Content Filtering"
description: "Automatically filter harmful, inappropriate, or policy-violating content in AI requests and responses."
section: "Security"
order: 5
---

## What is Content Filtering?

Content filtering prevents your AI application from processing or generating harmful content, including:
- Hate speech and discrimination
- Violence and graphic content
- Sexual or adult content
- Self-harm and dangerous activities
- Illegal activities

## Filter Categories

| Category | Description | Examples |
|----------|-------------|----------|
| **Hate Speech** | Attacks based on identity | Racial slurs, religious attacks |
| **Violence** | Graphic or threatening content | Violent threats, gore |
| **Sexual Content** | Adult or explicit material | NSFW imagery, explicit text |
| **Self-Harm** | Dangerous behavior encouragement | Suicide methods, self-injury |
| **Illegal Activity** | Instructions for crimes | Drug manufacturing, theft |
| **Profanity** | Offensive language | Curse words, slurs |

## How Content Filtering Works

1. **Input Scanning**: Before sending to the AI provider, Cencori scans the user's prompt for harmful content.
2. **Classification**: ML models categorize content and assign severity scores (low, medium, high, critical).
3. **Policy Enforcement**: Based on your configured policy, the request is either blocked, flagged, or allowed with warnings.
4. **Output Monitoring**: AI responses are also scanned. If harmful content is generated, it's blocked before reaching the user.

## Filtering Policy Modes

| Mode | Behavior | Use Case |
|------|----------|----------|
| **Strict** | Block all harmful content | Public apps, children's apps |
| **Moderate** | Block high/critical only | General purpose apps |
| **Permissive** | Log only, don't block | Internal tools, research |
| **Custom** | Define your own rules | Enterprise use cases |

## When Content is Blocked

```json
{
  "error": "Content violation detected",
  "code": "CONTENT_FILTER_VIOLATION",
  "status": 403,
  "details": {
    "categories": ["violence", "illegal_activity"],
    "severity": "high",
    "incident_id": "inc_filter_123"
  }
}
```

## Handling Content Filter Violations in Code

```typescript
try {
  const response = await cencori.ai.chat({
    model: 'gpt-4o',
    messages: [{ role: 'user', content: userInput }],
  });
  
  return response.content;
} catch (error: any) {
  if (error.code === 'CONTENT_FILTER_VIOLATION') {
    // Log the incident
    console.warn('Content filter triggered:', error.details);
    
    // Return user-friendly message
    return {
      error: 'Your message violates our content policy. Please rephrase.',
      categories: error.details.categories,
      severity: error.details.severity
    };
  }
  
  throw error;
}
```

## Custom Filtering Rules (Enterprise)

Enterprise customers can define custom rules:
- Industry-specific terms (e.g., medical terminology that's acceptable in healthcare)
- Company-specific blocklists
- Domain-specific allowlists
- Regional language variations

## Monitoring Filter Activity

View all content filter incidents in your dashboard:

1. Navigate to project dashboard
2. Click "Security" sidebar
3. Filter by "Content Filter Violation"
4. View breakdown by:
   - Category (violence, hate speech, etc.)
   - Severity level
   - Trends over time

## Best Practices

- Start with Moderate mode and adjust based on your app's audience.
- Show clear error messages to users explaining policy violations.
- Review filter incidents weekly to identify abuse patterns.
- For creative writing apps, consider Permissive mode with output filtering.
- Test edge cases with your specific content.
- Combine with prompt injection protection for comprehensive security.
