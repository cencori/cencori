---
title: "Semantic Caching"
description: "Reduce latency and costs by caching identical AI responses."
section: "platform"
order: 6
---

## Overview

Cencori implements **Semantic Caching** for AI completions. When enabled, the gateway checks if an identical request has been made recently. If a match is found, the cached response is returned immediately, bypassing the upstream AI provider.

### Benefits

1.  **Lower Latency**: Cached responses are served in milliseconds, compared to seconds for fresh AI generations.
2.  **Reduced Costs**: You are not charged by the AI provider (e.g., OpenAI, Anthropic) for cached hits.
3.  **Consistency**: Identical inputs yield identical outputs, ensuring stability for testing and deterministic workflows.

## How it Works

The cache key is generated using a SHA-256 hash of the following parameters:
-   `Project ID`
-   `Model` (e.g., `gpt-4o`, `ucla-v3`)
-   `Prompt` (exact text)
-   `Temperature`
-   `Max Tokens`

Any change to these parameters results in a new cache key (Cache MISS).

> [!NOTE]
> Currently, caching is enabled for all **non-streaming** requests by default. Streaming requests bypass the cache.

## Cache Headers

You can verify the cache status of any response using the `X-Cencori-Cache` header:

| Value | Description |
| :--- | :--- |
| `HIT` | The response was served from the cache. |
| `MISS` | The response was generated by the AI provider (and is now cached). |

## Retention (TTL)

Cached responses are stored for **1 hour** by default. After this period, the cache expires, and the next request will be treated as a fresh generation (MISS).

## Disabling Caching

*Coming Soon: You will be able to disable caching per-request via a custom header or query parameter.*
