---
title: "Comparisons"
description: "How Cencori compares to OpenRouter, LangChain, and Vercel AI SDK."
section: "Platform"
order: 10
---

A common question is: *"How is Cencori different from X?"*

The short answer: **Cencori is the Infrastructure Layer**. We are not just a model router, and we are not just a JS library. We are the cloud platform that powers your AI application.

## vs OpenRouter

**OpenRouter** is a pipe. **Cencori** is a platform.

| Feature | OpenRouter | Cencori |
| :--- | :--- | :--- |
| **Model Routing** | <Check /> Yes | <Check /> Yes (14+ Providers) |
| **Unified API** | <Check /> Yes | <Check /> Yes (OpenAI Compatible) |
| **Integrations** | <X /> No | <Check /> Vercel, Zapier, Supabase |
| **Security** | <X /> No | <Check /> PII Redaction, Prompt Injection |
| **Memory** | <X /> No | <Check /> Adaptive Memory (Vector Store) |
| **Workflows** | <X /> No | <Check /> Agent Orchestration |

**Summary**: Use OpenRouter if you just need access to a specific model. Use Cencori if you are building a production application that needs security, memory, and reliability.

## vs LangChain / LangGraph

**LangChain** is a library you have to host. **Cencori** is a serverless backend.

| Feature | LangChain | Cencori |
| :--- | :--- | :--- |
| **Model Routing** | <Check /> Yes (via Adapter) | <Check /> Yes (Native) |
| **Unified API** | <Check /> Yes | <Check /> Yes |
| **Integrations** | <Check /> Yes (Code) | <Check /> Yes (Native & Code) |
| **Security** | <X /> No | <Check /> PII Redaction, Prompt Injection |
| **Memory** | <AlertTriangle /> Partial (Self-Hosted) | <Check /> Adaptive Memory (Managed) |
| **Workflows** | <Check /> Yes (LangGraph) | <Check /> Agent Orchestration (Serverless) |
| **Hosting** | Self-hosted (e.g. EC2, Fargate) | Serverless Cloud |
| **State** | Redis/Postgres (Manual) | Built-in Persistence |
| **Retries** | You code it | Built-in (Exponential Backoff) |
| **Observability** | LangSmith (Separate) | Integrated Dashboard |

**Summary**: You can actually **use LangChain with Cencori**. Use LangChain for the graph logic, and let Cencori handle the LLM execution, memory storage, and observability.

## vs Vercel AI SDK

**Vercel AI SDK** is the Frontend. **Cencori** is the Backend.

| Feature | Vercel AI SDK | Cencori |
| :--- | :--- | :--- |
| **Model Routing** | <Check /> Yes (Client-side) | <Check /> Yes (Server-side) |
| **Unified API** | <Check /> Yes | <Check /> Yes |
| **Integrations** | <Check /> Yes (Frontend) | <Check /> Yes (Backend) |
| **Security** | <X /> No (Client-side only) | <Check /> Enterprise-grade (PII, Injection) |
| **Memory** | <X /> No | <Check /> Vector Store + Adaptive |
| **Workflows** | <X /> No | <Check /> Agent Orchestration |
| **Role** | Frontend Library | Backend Engine |
| **Focus** | UI State & Streaming | Intelligence & Security |

**Better Together**:
```typescript
import { cencori } from '@cencori/ai-sdk';
import { streamText } from 'ai';

// Vercel handles the Streaming & UI
const result = await streamText({
  // Cencori handles the Intelligence, Security, & Observability
  model: cencori('gpt-4o'),
  messages
});
```

## vs Vercel AI Gateway

**Vercel AI Gateway** focuses on caching and rate limiting. **Cencori** focuses on **Intelligence**.

| Feature | Vercel AI Gateway | Cencori |
| :--- | :--- | :--- |
| **Model Routing** | <Check /> Yes | <Check /> Yes |
| **Unified API** | <Check /> Yes | <Check /> Yes |
| **Integrations** | <AlertTriangle /> Limited | <Check /> Vercel, Zapier, Supabase |
| **Security** | <AlertTriangle /> Basic (Firewall) | <Check /> AI-Specific (PII, Jailbreak) |
| **Memory** | <X /> Stateless | <Check /> Stateful (User Memory) |
| **Workflows** | <X /> No | <Check /> Agent Orchestration |
| **Primary Goal** | Caching & Rate Limiting | Agent Orchestration |
| **Intelligence** | Passive (Network Layer) | Active (Application Layer) |

**Summary**: Cencori is an **Active** intelligence layer, whereas Vercel AI Gateway is a **Passive** network layer.

## vs LiteLLM

**LiteLLM** is a Python library/proxy. **Cencori** is a managed cloud platform.

| Feature | LiteLLM | Cencori |
| :--- | :--- | :--- |
| **Model Routing** | <Check /> Yes (Python Proxy) | <Check /> Yes (Global Edge) |
| **Unified API** | <Check /> Yes | <Check /> Yes |
| **Integrations** | <X /> No | <Check /> Vercel, Zapier, Supabase |
| **Security** | <AlertTriangle /> Basic (API Key Mgmt) | <Check /> Enterprise-grade (PII, Injection) |
| **Memory** | <X /> No | <Check /> Adaptive Memory |
| **Workflows** | <X /> No | <Check /> Agent Orchestration |
| **Hosting** | Self-hosted | Serverless Cloud |

**Summary**: LiteLLM is great for standardizing APIs if you want to manage your own proxy. Cencori provides the same standardization but adds managed infrastructure, security, and memory.

## vs Portkey / Helicone

**Portkey & Helicone** are primarily **Observability Gateways**. **Cencori** is an **Intelligence Platform**.

| Feature | Portkey / Helicone | Cencori |
| :--- | :--- | :--- |
| **Model Routing** | <Check /> Yes | <Check /> Yes |
| **Unified API** | <Check /> Yes | <Check /> Yes |
| **Integrations** | <AlertTriangle /> Observability-Focused | <Check /> Application-Focused |
| **Security** | <AlertTriangle /> Audit Logs Only | <Check /> Active Redaction & Blocking |
| **Memory** | <X /> No | <Check /> Adaptive Memory |
| **Workflows** | <X /> No | <Check /> Agent Orchestration |
| **Primary Goal** | Logging & Analytics | Building Agents |

**Summary**: Portkey and Helicone tell you *what happened* (Logging). Cencori helps you *make it happen* (Agents, Memory, Security).

## vs Mastra

**Mastra** is a TypeScript framework. **Cencori** is the infrastructure that powers it.

| Feature | Mastra | Cencori |
| :--- | :--- | :--- |
| **Model Routing** | <Check /> Yes (Local) | <Check /> Yes (Cloud) |
| **Unified API** | <Check /> Yes | <Check /> Yes |
| **Integrations** | <Check /> Yes (Code) | <Check /> Yes (Native) |
| **Security** | <X /> No | <Check /> PII Redaction, Prompt Injection |
| **Memory** | <AlertTriangle /> Local / Postgres | <Check /> Managed Vector Store |
| **Workflows** | <Check /> Yes (Local Execution) | <Check /> Serverless Execution |
| **Hosting** | Self-hosted | Serverless Cloud |
| **State** | Postgres (Manual) | Built-in Persistence |

**Summary**: Mastra is like "Next.js for Agents" (the framework). Cencori is the "Vercel for Agents" (the platform). You can use them together, or let Cencori handle the backend complexity entirely.
