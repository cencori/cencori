---
title: "Chat Endpoint"
description: "Conversational AI with streaming, tool calling, and structured output."
section: "AI Endpoints"
order: 1
---

The chat endpoint provides conversational AI capabilities with support for streaming, tool calling, and multiple providers.

## Basic Request

```typescript
const response = await cencori.ai.chat({
  model: 'gpt-4o',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Hello!' }
  ]
});

console.log(response.content);
```

## Request Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `model` | string | Yes | Model identifier |
| `messages` | array | Yes | Conversation messages |
| `temperature` | number | No | Randomness (0-2) |
| `maxTokens` | number | No | Maximum output tokens |
| `stream` | boolean | No | Enable streaming |
| `tools` | array | No | Available functions |

## Response

```typescript
{
  id: 'chatcmpl-...',
  content: 'Hello! How can I help you today?',
  model: 'gpt-4o',
  finishReason: 'stop',
  usage: {
    promptTokens: 15,
    completionTokens: 10,
    totalTokens: 25
  },
  toolCalls: null
}
```

## Streaming

```typescript
const stream = cencori.ai.chatStream({
  model: 'claude-3-5-sonnet-latest',
  messages: [{ role: 'user', content: 'Tell me a story' }]
});

for await (const chunk of stream) {
  process.stdout.write(chunk.delta);
}
```

## HTTP API

```bash
curl -X POST https://cencori.com/api/ai/chat \
  -H "CENCORI_API_KEY: csk_..." \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Supported Models

| Provider | Models |
|----------|--------|
| OpenAI | gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo |
| Anthropic | claude-3-5-sonnet-latest, claude-3-5-haiku-latest, claude-3-opus-latest, claude-3-haiku-20240307 |
| Google | gemini-2.0-flash, gemini-1.5-pro, gemini-1.5-flash |
| xAI | grok-2, grok-beta |
| DeepSeek | deepseek-chat, deepseek-reasoner |
| Groq | llama-3.1-8b-instant, llama-3.1-70b-versatile |
| Mistral | mistral-large-latest, mistral-small-latest |
| Together | meta-llama/Llama-3-8b-chat-hf, meta-llama/Llama-3-70b-chat-hf |
| Perplexity | llama-3.1-sonar-small-128k-online, llama-3.1-sonar-large-128k-online |
| OpenRouter | meta-llama/llama-3.1-8b-instruct |
| Qwen | qwen-turbo, qwen-plus |
| Cohere | command-r, command-r-plus |
