{"version":3,"sources":["../src/ai/index.ts","../src/compute/index.ts","../src/workflow/index.ts","../src/storage/index.ts","../src/cencori.ts","../src/vercel/cencori-chat-model.ts","../src/vercel/cencori-provider.ts"],"sourcesContent":["/**\n * AI Gateway - Chat, Completions, and Embeddings\n * \n * @example\n * const response = await cencori.ai.chat({\n *   model: 'gpt-4o',\n *   messages: [{ role: 'user', content: 'Hello!' }]\n * });\n */\n\nimport type {\n    CencoriConfig,\n    ChatRequest,\n    ChatResponse,\n    CompletionRequest,\n    EmbeddingRequest,\n    EmbeddingResponse\n} from '../types';\n\n// API Response types\ninterface OpenAIChatResponse {\n    id: string;\n    model: string;\n    choices?: Array<{\n        message?: {\n            content?: string;\n        };\n    }>;\n    usage?: {\n        prompt_tokens?: number;\n        completion_tokens?: number;\n        total_tokens?: number;\n    };\n}\n\ninterface OpenAIEmbeddingResponse {\n    model: string;\n    data?: Array<{\n        embedding: number[];\n    }>;\n    usage?: {\n        total_tokens?: number;\n    };\n}\n\nexport class AINamespace {\n    private config: Required<CencoriConfig>;\n\n    constructor(config: Required<CencoriConfig>) {\n        this.config = config;\n    }\n\n    /**\n     * Create a chat completion\n     * \n     * @example\n     * const response = await cencori.ai.chat({\n     *   model: 'gpt-4o',\n     *   messages: [{ role: 'user', content: 'Hello!' }]\n     * });\n     */\n    async chat(request: ChatRequest): Promise<ChatResponse> {\n        const response = await fetch(`${this.config.baseUrl}/api/v1/chat/completions`, {\n            method: 'POST',\n            headers: {\n                'Authorization': `Bearer ${this.config.apiKey}`,\n                'Content-Type': 'application/json',\n                ...this.config.headers,\n            },\n            body: JSON.stringify({\n                model: request.model,\n                messages: request.messages,\n                temperature: request.temperature,\n                max_tokens: request.maxTokens,\n                stream: request.stream ?? false,\n            }),\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json().catch(() => ({ error: 'Unknown error' })) as { error?: string };\n            throw new Error(`Cencori API error: ${errorData.error || response.statusText}`);\n        }\n\n        const data = await response.json() as OpenAIChatResponse;\n\n        return {\n            id: data.id,\n            model: data.model,\n            content: data.choices?.[0]?.message?.content ?? '',\n            usage: {\n                promptTokens: data.usage?.prompt_tokens ?? 0,\n                completionTokens: data.usage?.completion_tokens ?? 0,\n                totalTokens: data.usage?.total_tokens ?? 0,\n            },\n        };\n    }\n\n    /**\n     * Create a text completion\n     * \n     * @example\n     * const response = await cencori.ai.completions({\n     *   model: 'gpt-4o',\n     *   prompt: 'Write a haiku about coding'\n     * });\n     */\n    async completions(request: CompletionRequest): Promise<ChatResponse> {\n        // Convert to chat format internally\n        return this.chat({\n            model: request.model,\n            messages: [{ role: 'user', content: request.prompt }],\n            temperature: request.temperature,\n            maxTokens: request.maxTokens,\n        });\n    }\n\n    /**\n     * Create embeddings\n     * \n     * @example\n     * const response = await cencori.ai.embeddings({\n     *   model: 'text-embedding-3-small',\n     *   input: 'Hello world'\n     * });\n     */\n    async embeddings(request: EmbeddingRequest): Promise<EmbeddingResponse> {\n        const response = await fetch(`${this.config.baseUrl}/api/v1/embeddings`, {\n            method: 'POST',\n            headers: {\n                'Authorization': `Bearer ${this.config.apiKey}`,\n                'Content-Type': 'application/json',\n                ...this.config.headers,\n            },\n            body: JSON.stringify({\n                model: request.model,\n                input: request.input,\n            }),\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json().catch(() => ({ error: 'Unknown error' })) as { error?: string };\n            throw new Error(`Cencori API error: ${errorData.error || response.statusText}`);\n        }\n\n        const data = await response.json() as OpenAIEmbeddingResponse;\n\n        return {\n            model: data.model,\n            embeddings: data.data?.map((d) => d.embedding) ?? [],\n            usage: {\n                totalTokens: data.usage?.total_tokens ?? 0,\n            },\n        };\n    }\n}\n","/**\n * Compute Namespace - Serverless Functions & GPU Access\n * \n * ðŸš§ Coming Soon\n * \n * @example\n * const result = await cencori.compute.run('my-function', { \n *   input: { data: 'hello' } \n * });\n */\n\nimport type { ComputeRunOptions } from '../types';\n\nexport class ComputeNamespace {\n    /**\n     * Run a serverless function\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async run(functionId: string, options?: ComputeRunOptions): Promise<never> {\n        throw new Error(\n            `cencori.compute.run() is coming soon! ` +\n            `Function \"${functionId}\" cannot be executed yet. ` +\n            `Join our waitlist at https://cencori.com/compute`\n        );\n    }\n\n    /**\n     * Deploy a function\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async deploy(config: { name: string; code: string }): Promise<never> {\n        throw new Error(\n            `cencori.compute.deploy() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/compute`\n        );\n    }\n\n    /**\n     * List deployed functions\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async list(): Promise<never> {\n        throw new Error(\n            `cencori.compute.list() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/compute`\n        );\n    }\n}\n","/**\n * Workflow Namespace - AI Pipelines & Orchestration\n * \n * ðŸš§ Coming Soon\n * \n * @example\n * await cencori.workflow.trigger('data-enrichment', { \n *   data: { userId: '123' } \n * });\n */\n\nimport type { WorkflowTriggerOptions } from '../types';\n\nexport class WorkflowNamespace {\n    /**\n     * Trigger a workflow\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async trigger(workflowId: string, options?: WorkflowTriggerOptions): Promise<never> {\n        throw new Error(\n            `cencori.workflow.trigger() is coming soon! ` +\n            `Workflow \"${workflowId}\" cannot be triggered yet. ` +\n            `Join our waitlist at https://cencori.com/workflow`\n        );\n    }\n\n    /**\n     * Create a new workflow\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async create(config: { name: string; steps: unknown[] }): Promise<never> {\n        throw new Error(\n            `cencori.workflow.create() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/workflow`\n        );\n    }\n\n    /**\n     * Get workflow run status\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async status(runId: string): Promise<never> {\n        throw new Error(\n            `cencori.workflow.status() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/workflow`\n        );\n    }\n\n    /**\n     * List workflows\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async list(): Promise<never> {\n        throw new Error(\n            `cencori.workflow.list() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/workflow`\n        );\n    }\n}\n","/**\n * Storage Namespace - Vector Database, Knowledge Base, RAG\n * \n * ðŸš§ Coming Soon\n * \n * @example\n * const results = await cencori.storage.vectors.search('query', { limit: 5 });\n */\n\nimport type { VectorSearchOptions } from '../types';\n\nclass VectorsNamespace {\n    /**\n     * Search vectors by query\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async search(query: string, options?: VectorSearchOptions): Promise<never> {\n        throw new Error(\n            `cencori.storage.vectors.search() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/storage`\n        );\n    }\n\n    /**\n     * Upsert vectors\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async upsert(vectors: { id: string; values: number[]; metadata?: Record<string, unknown> }[]): Promise<never> {\n        throw new Error(\n            `cencori.storage.vectors.upsert() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/storage`\n        );\n    }\n\n    /**\n     * Delete vectors by ID\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async delete(ids: string[]): Promise<never> {\n        throw new Error(\n            `cencori.storage.vectors.delete() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/storage`\n        );\n    }\n}\n\nclass KnowledgeNamespace {\n    /**\n     * Query the knowledge base\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async query(question: string): Promise<never> {\n        throw new Error(\n            `cencori.storage.knowledge.query() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/storage`\n        );\n    }\n\n    /**\n     * Add documents to knowledge base\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async add(documents: { content: string; metadata?: Record<string, unknown> }[]): Promise<never> {\n        throw new Error(\n            `cencori.storage.knowledge.add() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/storage`\n        );\n    }\n}\n\nclass FilesNamespace {\n    /**\n     * Upload a file\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async upload(file: Blob | Buffer, name: string): Promise<never> {\n        throw new Error(\n            `cencori.storage.files.upload() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/storage`\n        );\n    }\n\n    /**\n     * Process a file (extract text, OCR, etc.)\n     * \n     * ðŸš§ Coming Soon - This feature is not yet available.\n     */\n    async process(fileId: string): Promise<never> {\n        throw new Error(\n            `cencori.storage.files.process() is coming soon! ` +\n            `Join our waitlist at https://cencori.com/storage`\n        );\n    }\n}\n\nexport class StorageNamespace {\n    /**\n     * Vector database operations\n     */\n    readonly vectors = new VectorsNamespace();\n\n    /**\n     * Knowledge base operations (RAG)\n     */\n    readonly knowledge = new KnowledgeNamespace();\n\n    /**\n     * File storage and processing\n     */\n    readonly files = new FilesNamespace();\n}\n","/**\n * Cencori - Unified AI Infrastructure SDK\n * \n * One SDK for AI Gateway, Compute, Workflow, and Storage.\n * Every operation is secured, logged, and tracked.\n * \n * @example\n * import { Cencori } from 'cencori';\n * \n * const cencori = new Cencori({ apiKey: 'csk_...' });\n * \n * // AI Gateway\n * const response = await cencori.ai.chat({\n *   model: 'gpt-4o',\n *   messages: [{ role: 'user', content: 'Hello!' }]\n * });\n * \n * // Compute (coming soon)\n * await cencori.compute.run('my-function', { input: data });\n * \n * // Workflow (coming soon)\n * await cencori.workflow.trigger('pipeline-id', { data });\n * \n * // Storage (coming soon)\n * await cencori.storage.vectors.search('query');\n */\n\nimport type { CencoriConfig } from './types';\nimport { AINamespace } from './ai';\nimport { ComputeNamespace } from './compute';\nimport { WorkflowNamespace } from './workflow';\nimport { StorageNamespace } from './storage';\n\nconst DEFAULT_BASE_URL = 'https://cencori.com';\n\nexport class Cencori {\n    private config: Required<CencoriConfig>;\n\n    /**\n     * AI Gateway - Chat, completions, embeddings with security & observability\n     * \n     * @example\n     * await cencori.ai.chat({ model: 'gpt-4o', messages: [...] });\n     */\n    readonly ai: AINamespace;\n\n    /**\n     * Compute - Serverless functions & GPU access\n     * \n     * ðŸš§ Coming Soon\n     * \n     * @example\n     * await cencori.compute.run('my-function', { input: data });\n     */\n    readonly compute: ComputeNamespace;\n\n    /**\n     * Workflow - AI pipelines & orchestration\n     * \n     * ðŸš§ Coming Soon\n     * \n     * @example\n     * await cencori.workflow.trigger('pipeline-id', { data });\n     */\n    readonly workflow: WorkflowNamespace;\n\n    /**\n     * Storage - Vector database, knowledge base, RAG\n     * \n     * ðŸš§ Coming Soon\n     * \n     * @example\n     * await cencori.storage.vectors.search('query');\n     */\n    readonly storage: StorageNamespace;\n\n    /**\n     * Create a new Cencori client\n     * \n     * @param config - Configuration options\n     * @param config.apiKey - Your Cencori API key (starts with 'csk_')\n     * @param config.baseUrl - Custom API base URL (default: https://cencori.com)\n     * @param config.headers - Custom headers to include in requests\n     * \n     * @example\n     * const cencori = new Cencori({ \n     *   apiKey: process.env.CENCORI_API_KEY \n     * });\n     */\n    constructor(config: CencoriConfig = {}) {\n        const apiKey = config.apiKey ?? process.env.CENCORI_API_KEY;\n\n        if (!apiKey) {\n            throw new Error(\n                'Cencori API key is required. ' +\n                'Pass it via new Cencori({ apiKey: \"csk_...\" }) or set CENCORI_API_KEY environment variable.'\n            );\n        }\n\n        this.config = {\n            apiKey,\n            baseUrl: config.baseUrl ?? DEFAULT_BASE_URL,\n            headers: config.headers ?? {},\n        };\n\n        // Initialize namespaces\n        this.ai = new AINamespace(this.config);\n        this.compute = new ComputeNamespace();\n        this.workflow = new WorkflowNamespace();\n        this.storage = new StorageNamespace();\n    }\n\n    /**\n     * Get the current configuration (API key is masked)\n     */\n    getConfig(): { baseUrl: string; apiKeyHint: string } {\n        return {\n            baseUrl: this.config.baseUrl,\n            apiKeyHint: `${this.config.apiKey.slice(0, 6)}...${this.config.apiKey.slice(-4)}`,\n        };\n    }\n}\n\n// Export types\nexport type { CencoriConfig, ChatRequest, ChatResponse, ChatMessage } from './types';\nexport type { AINamespace } from './ai';\nexport type { ComputeNamespace } from './compute';\nexport type { WorkflowNamespace } from './workflow';\nexport type { StorageNamespace } from './storage';\n","/**\n * Cencori Chat Language Model\n * \n * Implements the Vercel AI SDK's LanguageModelV3 interface (AI SDK v6 compatible)\n */\n\nimport type {\n    LanguageModelV3,\n    LanguageModelV3CallOptions,\n    LanguageModelV3GenerateResult,\n    LanguageModelV3StreamResult,\n    LanguageModelV3StreamPart,\n    LanguageModelV3Content,\n    LanguageModelV3Usage,\n    LanguageModelV3FinishReason,\n    SharedV3Warning,\n} from '@ai-sdk/provider';\n\nexport interface CencoriChatModelSettings {\n    apiKey: string;\n    baseUrl: string;\n    headers?: Record<string, string>;\n    userId?: string;\n}\n\ninterface CencoriMessage {\n    role: 'system' | 'user' | 'assistant' | 'tool';\n    content: string;\n    toolCallId?: string;\n}\n\n/**\n * Tool definition in Cencori format (OpenAI-compatible)\n */\ninterface CencoriTool {\n    type: 'function';\n    function: {\n        name: string;\n        description: string;\n        parameters: Record<string, any>;\n    };\n}\n\n/**\n * Tool call from the model\n */\ninterface CencoriToolCall {\n    id: string;\n    type: 'function';\n    function: {\n        name: string;\n        arguments: string;\n    };\n}\n\ninterface CencoriResponse {\n    content: string;\n    model: string;\n    provider: string;\n    usage: {\n        prompt_tokens: number;\n        completion_tokens: number;\n        total_tokens: number;\n    };\n    cost_usd: number;\n    finish_reason?: string;\n    tool_calls?: CencoriToolCall[];\n}\n\ninterface CencoriStreamChunk {\n    delta: string;\n    finish_reason?: string;\n    tool_calls?: CencoriToolCall[];\n}\n\nexport class CencoriChatLanguageModel implements LanguageModelV3 {\n    readonly specificationVersion = 'v3' as const;\n    readonly provider = 'cencori';\n\n    readonly modelId: string;\n    readonly supportedUrls: Record<string, RegExp[]> = {};\n    private readonly settings: CencoriChatModelSettings;\n\n    constructor(modelId: string, settings: CencoriChatModelSettings) {\n        this.modelId = modelId;\n        this.settings = settings;\n    }\n\n    private getHeaders(): Record<string, string> {\n        return {\n            'Content-Type': 'application/json',\n            'CENCORI_API_KEY': this.settings.apiKey,\n            ...this.settings.headers,\n        };\n    }\n\n    private convertMessages(options: LanguageModelV3CallOptions): CencoriMessage[] {\n        const messages: CencoriMessage[] = [];\n\n        // In V3, options.prompt is directly an array of LanguageModelV3Message\n        const promptMessages = options.prompt;\n\n        if (!promptMessages || !Array.isArray(promptMessages)) {\n            return messages;\n        }\n\n        for (const msg of promptMessages) {\n            let content = '';\n\n            if (msg.role === 'system') {\n                // System messages have content as string directly\n                content = msg.content as string;\n            } else if (msg.role === 'user' || msg.role === 'assistant') {\n                // User and assistant messages have content as array of parts\n                const msgContent = msg.content;\n                if (Array.isArray(msgContent)) {\n                    content = msgContent\n                        .filter((part: { type: string }) => part.type === 'text')\n                        .map((part: { type: string; text?: string }) => part.text || '')\n                        .join('');\n                } else if (typeof msgContent === 'string') {\n                    content = msgContent;\n                }\n            }\n\n            if (content && (msg.role === 'system' || msg.role === 'user' || msg.role === 'assistant')) {\n                messages.push({\n                    role: msg.role as 'system' | 'user' | 'assistant',\n                    content,\n                });\n            }\n        }\n\n        return messages;\n    }\n\n    private mapFinishReason(reason?: string): LanguageModelV3FinishReason {\n        let unified: 'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other';\n\n        switch (reason) {\n            case 'stop':\n            case 'end_turn':\n                unified = 'stop';\n                break;\n            case 'length':\n            case 'max_tokens':\n                unified = 'length';\n                break;\n            case 'content_filter':\n                unified = 'content-filter';\n                break;\n            case 'tool_calls':\n            case 'tool-calls':\n                unified = 'tool-calls';\n                break;\n            case 'error':\n                unified = 'error';\n                break;\n            default:\n                unified = 'stop';\n        }\n\n        return { unified, raw: reason };\n    }\n\n    private buildUsage(inputTokens: number, outputTokens: number): LanguageModelV3Usage {\n        return {\n            inputTokens: {\n                total: inputTokens,\n                noCache: inputTokens,\n                cacheRead: undefined,\n                cacheWrite: undefined,\n            },\n            outputTokens: {\n                total: outputTokens,\n                text: outputTokens,\n                reasoning: undefined,\n            },\n        };\n    }\n\n    /**\n     * Convert Vercel AI SDK tools to Cencori format\n     */\n    private convertTools(options: LanguageModelV3CallOptions): CencoriTool[] | undefined {\n        if (!options.tools || options.tools.length === 0) {\n            return undefined;\n        }\n\n        return options.tools\n            .filter(t => t.type === 'function')\n            .map(t => ({\n                type: 'function' as const,\n                function: {\n                    name: t.name,\n                    description: t.description || '',\n                    parameters: t.inputSchema as Record<string, any>,\n                },\n            }));\n    }\n\n    /**\n     * Convert Vercel AI SDK tool choice to Cencori format\n     */\n    private convertToolChoice(options: LanguageModelV3CallOptions): string | { type: 'function'; function: { name: string } } | undefined {\n        const tc = options.toolChoice;\n        if (!tc) return undefined;\n\n        switch (tc.type) {\n            case 'auto':\n                return 'auto';\n            case 'none':\n                return 'none';\n            case 'required':\n                return 'required';\n            case 'tool':\n                return { type: 'function', function: { name: tc.toolName } };\n            default:\n                return undefined;\n        }\n    }\n\n    async doGenerate(options: LanguageModelV3CallOptions): Promise<LanguageModelV3GenerateResult> {\n        const messages = this.convertMessages(options);\n        const tools = this.convertTools(options);\n        const toolChoice = this.convertToolChoice(options);\n\n        const response = await fetch(`${this.settings.baseUrl}/api/ai/chat`, {\n            method: 'POST',\n            headers: this.getHeaders(),\n            body: JSON.stringify({\n                messages,\n                model: this.modelId,\n                temperature: options.temperature,\n                maxTokens: options.maxOutputTokens,\n                stream: false,\n                userId: this.settings.userId,\n                tools,\n                toolChoice,\n            }),\n            signal: options.abortSignal,\n        });\n\n        if (!response.ok) {\n            const error = await response.json().catch(() => ({ error: 'Unknown error' })) as { error?: string };\n            throw new Error(`Cencori API error: ${error.error || response.statusText}`);\n        }\n\n        const data = await response.json() as CencoriResponse;\n\n        // Build content array\n        const content: LanguageModelV3Content[] = [];\n\n        // Add text content if present\n        if (data.content) {\n            content.push({\n                type: 'text',\n                text: data.content,\n                providerMetadata: undefined,\n            });\n        }\n\n        // Add tool calls if present\n        if (data.tool_calls && data.tool_calls.length > 0) {\n            for (const tc of data.tool_calls) {\n                content.push({\n                    type: 'tool-call',\n                    toolCallId: tc.id,\n                    toolName: tc.function.name,\n                    input: tc.function.arguments,\n                    providerMetadata: undefined,\n                });\n            }\n        }\n\n        const warnings: SharedV3Warning[] = [];\n\n        return {\n            content,\n            finishReason: this.mapFinishReason(data.finish_reason),\n            usage: this.buildUsage(data.usage.prompt_tokens, data.usage.completion_tokens),\n            warnings,\n        };\n    }\n\n    async doStream(options: LanguageModelV3CallOptions): Promise<LanguageModelV3StreamResult> {\n        const messages = this.convertMessages(options);\n        const tools = this.convertTools(options);\n        const toolChoice = this.convertToolChoice(options);\n        const self = this;\n\n        const response = await fetch(`${this.settings.baseUrl}/api/ai/chat`, {\n            method: 'POST',\n            headers: this.getHeaders(),\n            body: JSON.stringify({\n                messages,\n                model: this.modelId,\n                temperature: options.temperature,\n                maxTokens: options.maxOutputTokens,\n                stream: true,\n                userId: this.settings.userId,\n                tools,\n                toolChoice,\n            }),\n            signal: options.abortSignal,\n        });\n\n        if (!response.ok) {\n            const error = await response.json().catch(() => ({ error: 'Unknown error' })) as { error?: string };\n            throw new Error(`Cencori API error: ${error.error || response.statusText}`);\n        }\n\n        const reader = response.body?.getReader();\n        if (!reader) {\n            throw new Error('Response body is null');\n        }\n\n        const decoder = new TextDecoder();\n        let buffer = '';\n        let inputTokens = 0;\n        let outputTokens = 0;\n        const textPartId = 'text-0';\n        let started = false;\n\n        const stream = new ReadableStream<LanguageModelV3StreamPart>({\n            async pull(controller) {\n                try {\n                    const { done, value } = await reader.read();\n\n                    if (done) {\n                        // End text block and finish\n                        if (started) {\n                            controller.enqueue({\n                                type: 'text-end',\n                                id: textPartId,\n                            });\n                        }\n                        controller.enqueue({\n                            type: 'finish',\n                            finishReason: self.mapFinishReason('stop'),\n                            usage: self.buildUsage(inputTokens, outputTokens),\n                        });\n                        controller.close();\n                        return;\n                    }\n\n                    buffer += decoder.decode(value, { stream: true });\n                    const lines = buffer.split('\\n');\n                    buffer = lines.pop() || '';\n\n                    for (const line of lines) {\n                        if (line.trim() === '') continue;\n                        if (!line.startsWith('data: ')) continue;\n\n                        const data = line.slice(6);\n                        if (data === '[DONE]') {\n                            if (started) {\n                                controller.enqueue({\n                                    type: 'text-end',\n                                    id: textPartId,\n                                });\n                            }\n                            controller.enqueue({\n                                type: 'finish',\n                                finishReason: self.mapFinishReason('stop'),\n                                usage: self.buildUsage(inputTokens, outputTokens),\n                            });\n                            controller.close();\n                            return;\n                        }\n\n                        try {\n                            const chunk = JSON.parse(data) as CencoriStreamChunk;\n\n                            // Handle text delta\n                            if (chunk.delta) {\n                                // Start text if not started\n                                if (!started) {\n                                    started = true;\n                                    controller.enqueue({\n                                        type: 'text-start',\n                                        id: textPartId,\n                                    });\n                                }\n\n                                outputTokens += Math.ceil(chunk.delta.length / 4); // Rough estimate\n                                controller.enqueue({\n                                    type: 'text-delta',\n                                    id: textPartId,\n                                    delta: chunk.delta,\n                                });\n                            }\n\n                            // Handle tool calls\n                            if (chunk.tool_calls && chunk.tool_calls.length > 0) {\n                                for (const tc of chunk.tool_calls) {\n                                    // Emit complete tool-call event\n                                    controller.enqueue({\n                                        type: 'tool-call',\n                                        toolCallId: tc.id,\n                                        toolName: tc.function.name,\n                                        input: tc.function.arguments,\n                                        providerMetadata: undefined,\n                                    });\n                                }\n                            }\n\n                            if (chunk.finish_reason) {\n                                if (started) {\n                                    controller.enqueue({\n                                        type: 'text-end',\n                                        id: textPartId,\n                                    });\n                                }\n                                controller.enqueue({\n                                    type: 'finish',\n                                    finishReason: self.mapFinishReason(chunk.finish_reason),\n                                    usage: self.buildUsage(inputTokens, outputTokens),\n                                });\n                                controller.close();\n                                return;\n                            }\n                        } catch {\n                            // Skip malformed JSON\n                        }\n                    }\n                } catch (error) {\n                    controller.error(error);\n                }\n            },\n            cancel() {\n                reader.cancel();\n            },\n        });\n\n        return {\n            stream,\n        };\n    }\n}\n","/**\n * Cencori AI Provider for Vercel AI SDK\n * \n * Use Cencori with streamText(), generateText(), and useChat()\n */\n\nimport { CencoriChatLanguageModel } from './cencori-chat-model';\nimport type { CencoriProviderSettings, CencoriChatSettings } from './types';\n\nexport interface CencoriProvider {\n    /**\n     * Create a Cencori chat model for use with Vercel AI SDK\n     * \n     * @param modelId - The model ID (e.g., 'gemini-2.5-flash', 'gpt-4o', 'claude-3-opus')\n     * @param settings - Optional model-specific settings\n     * @returns A LanguageModelV1 compatible model\n     * \n     * @example\n     * import { cencori } from 'cencori';\n     * import { streamText } from 'ai';\n     * \n     * const result = await streamText({\n     *   model: cencori('gemini-2.5-flash'),\n     *   messages: [{ role: 'user', content: 'Hello!' }]\n     * });\n     */\n    (modelId: string, settings?: CencoriChatSettings): CencoriChatLanguageModel;\n\n    /**\n     * Create a chat model (alias for the provider function)\n     */\n    chat: (modelId: string, settings?: CencoriChatSettings) => CencoriChatLanguageModel;\n}\n\n/**\n * Create a Cencori provider instance\n * \n * @param options - Provider configuration options\n * @returns A Cencori provider\n * \n * @example\n * import { createCencori } from 'cencori';\n * \n * const cencori = createCencori({\n *   apiKey: process.env.CENCORI_API_KEY\n * });\n * \n * const result = await streamText({\n *   model: cencori('gemini-2.5-flash'),\n *   messages: [{ role: 'user', content: 'Hello!' }]\n * });\n */\nexport function createCencori(options: CencoriProviderSettings = {}): CencoriProvider {\n    const baseUrl = options.baseUrl ?? 'https://cencori.com';\n    const apiKey = options.apiKey ?? process.env.CENCORI_API_KEY;\n\n    if (!apiKey) {\n        throw new Error('Cencori API key is required. Pass it via options.apiKey or set CENCORI_API_KEY environment variable.');\n    }\n\n    const createModel = (modelId: string, settings: CencoriChatSettings = {}) => {\n        return new CencoriChatLanguageModel(modelId, {\n            apiKey,\n            baseUrl,\n            headers: options.headers,\n            ...settings,\n        });\n    };\n\n    const provider = function (modelId: string, settings?: CencoriChatSettings) {\n        return createModel(modelId, settings);\n    } as CencoriProvider;\n\n    provider.chat = createModel;\n\n    return provider;\n}\n\n/**\n * Default Cencori provider instance\n * Uses CENCORI_API_KEY environment variable (lazy initialization)\n * \n * @example\n * import { cencori } from 'cencori';\n * import { streamText } from 'ai';\n * \n * const result = await streamText({\n *   model: cencori('gemini-2.5-flash'),\n *   messages: [{ role: 'user', content: 'Hello!' }]\n * });\n */\nexport const cencori: CencoriProvider = function (modelId: string, settings?: CencoriChatSettings) {\n    const apiKey = process.env.CENCORI_API_KEY;\n    if (!apiKey) {\n        throw new Error('CENCORI_API_KEY environment variable is required. Set it or use createCencori({ apiKey: \"...\" }) instead.');\n    }\n    return new CencoriChatLanguageModel(modelId, {\n        apiKey,\n        baseUrl: 'https://cencori.com',\n        ...settings,\n    });\n} as CencoriProvider;\n\ncencori.chat = cencori;\n"],"mappings":";AA6CO,IAAM,cAAN,MAAkB;AAAA,EAGrB,YAAY,QAAiC;AACzC,SAAK,SAAS;AAAA,EAClB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAWA,MAAM,KAAK,SAA6C;AACpD,UAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,OAAO,4BAA4B;AAAA,MAC3E,QAAQ;AAAA,MACR,SAAS;AAAA,QACL,iBAAiB,UAAU,KAAK,OAAO,MAAM;AAAA,QAC7C,gBAAgB;AAAA,QAChB,GAAG,KAAK,OAAO;AAAA,MACnB;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACjB,OAAO,QAAQ;AAAA,QACf,UAAU,QAAQ;AAAA,QAClB,aAAa,QAAQ;AAAA,QACrB,YAAY,QAAQ;AAAA,QACpB,QAAQ,QAAQ,UAAU;AAAA,MAC9B,CAAC;AAAA,IACL,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AACd,YAAM,YAAY,MAAM,SAAS,KAAK,EAAE,MAAM,OAAO,EAAE,OAAO,gBAAgB,EAAE;AAChF,YAAM,IAAI,MAAM,sBAAsB,UAAU,SAAS,SAAS,UAAU,EAAE;AAAA,IAClF;AAEA,UAAM,OAAO,MAAM,SAAS,KAAK;AAEjC,WAAO;AAAA,MACH,IAAI,KAAK;AAAA,MACT,OAAO,KAAK;AAAA,MACZ,SAAS,KAAK,UAAU,CAAC,GAAG,SAAS,WAAW;AAAA,MAChD,OAAO;AAAA,QACH,cAAc,KAAK,OAAO,iBAAiB;AAAA,QAC3C,kBAAkB,KAAK,OAAO,qBAAqB;AAAA,QACnD,aAAa,KAAK,OAAO,gBAAgB;AAAA,MAC7C;AAAA,IACJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAWA,MAAM,YAAY,SAAmD;AAEjE,WAAO,KAAK,KAAK;AAAA,MACb,OAAO,QAAQ;AAAA,MACf,UAAU,CAAC,EAAE,MAAM,QAAQ,SAAS,QAAQ,OAAO,CAAC;AAAA,MACpD,aAAa,QAAQ;AAAA,MACrB,WAAW,QAAQ;AAAA,IACvB,CAAC;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAWA,MAAM,WAAW,SAAuD;AACpE,UAAM,WAAW,MAAM,MAAM,GAAG,KAAK,OAAO,OAAO,sBAAsB;AAAA,MACrE,QAAQ;AAAA,MACR,SAAS;AAAA,QACL,iBAAiB,UAAU,KAAK,OAAO,MAAM;AAAA,QAC7C,gBAAgB;AAAA,QAChB,GAAG,KAAK,OAAO;AAAA,MACnB;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACjB,OAAO,QAAQ;AAAA,QACf,OAAO,QAAQ;AAAA,MACnB,CAAC;AAAA,IACL,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AACd,YAAM,YAAY,MAAM,SAAS,KAAK,EAAE,MAAM,OAAO,EAAE,OAAO,gBAAgB,EAAE;AAChF,YAAM,IAAI,MAAM,sBAAsB,UAAU,SAAS,SAAS,UAAU,EAAE;AAAA,IAClF;AAEA,UAAM,OAAO,MAAM,SAAS,KAAK;AAEjC,WAAO;AAAA,MACH,OAAO,KAAK;AAAA,MACZ,YAAY,KAAK,MAAM,IAAI,CAAC,MAAM,EAAE,SAAS,KAAK,CAAC;AAAA,MACnD,OAAO;AAAA,QACH,aAAa,KAAK,OAAO,gBAAgB;AAAA,MAC7C;AAAA,IACJ;AAAA,EACJ;AACJ;;;AC7IO,IAAM,mBAAN,MAAuB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAM1B,MAAM,IAAI,YAAoB,SAA6C;AACvE,UAAM,IAAI;AAAA,MACN,mDACa,UAAU;AAAA,IAE3B;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAO,QAAwD;AACjE,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAuB;AACzB,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AACJ;;;ACrCO,IAAM,oBAAN,MAAwB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAM3B,MAAM,QAAQ,YAAoB,SAAkD;AAChF,UAAM,IAAI;AAAA,MACN,wDACa,UAAU;AAAA,IAE3B;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAO,QAA4D;AACrE,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAO,OAA+B;AACxC,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAuB;AACzB,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AACJ;;;ACnDA,IAAM,mBAAN,MAAuB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMnB,MAAM,OAAO,OAAe,SAA+C;AACvE,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAO,SAAiG;AAC1G,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAO,KAA+B;AACxC,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AACJ;AAEA,IAAM,qBAAN,MAAyB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMrB,MAAM,MAAM,UAAkC;AAC1C,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,IAAI,WAAsF;AAC5F,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AACJ;AAEA,IAAM,iBAAN,MAAqB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMjB,MAAM,OAAO,MAAqB,MAA8B;AAC5D,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,QAAgC;AAC1C,UAAM,IAAI;AAAA,MACN;AAAA,IAEJ;AAAA,EACJ;AACJ;AAEO,IAAM,mBAAN,MAAuB;AAAA,EAAvB;AAIH;AAAA;AAAA;AAAA,SAAS,UAAU,IAAI,iBAAiB;AAKxC;AAAA;AAAA;AAAA,SAAS,YAAY,IAAI,mBAAmB;AAK5C;AAAA;AAAA;AAAA,SAAS,QAAQ,IAAI,eAAe;AAAA;AACxC;;;ACnFA,IAAM,mBAAmB;AAElB,IAAM,UAAN,MAAc;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAsDjB,YAAY,SAAwB,CAAC,GAAG;AACpC,UAAM,SAAS,OAAO,UAAU,QAAQ,IAAI;AAE5C,QAAI,CAAC,QAAQ;AACT,YAAM,IAAI;AAAA,QACN;AAAA,MAEJ;AAAA,IACJ;AAEA,SAAK,SAAS;AAAA,MACV;AAAA,MACA,SAAS,OAAO,WAAW;AAAA,MAC3B,SAAS,OAAO,WAAW,CAAC;AAAA,IAChC;AAGA,SAAK,KAAK,IAAI,YAAY,KAAK,MAAM;AACrC,SAAK,UAAU,IAAI,iBAAiB;AACpC,SAAK,WAAW,IAAI,kBAAkB;AACtC,SAAK,UAAU,IAAI,iBAAiB;AAAA,EACxC;AAAA;AAAA;AAAA;AAAA,EAKA,YAAqD;AACjD,WAAO;AAAA,MACH,SAAS,KAAK,OAAO;AAAA,MACrB,YAAY,GAAG,KAAK,OAAO,OAAO,MAAM,GAAG,CAAC,CAAC,MAAM,KAAK,OAAO,OAAO,MAAM,EAAE,CAAC;AAAA,IACnF;AAAA,EACJ;AACJ;;;AC9CO,IAAM,2BAAN,MAA0D;AAAA,EAQ7D,YAAY,SAAiB,UAAoC;AAPjE,SAAS,uBAAuB;AAChC,SAAS,WAAW;AAGpB,SAAS,gBAA0C,CAAC;AAIhD,SAAK,UAAU;AACf,SAAK,WAAW;AAAA,EACpB;AAAA,EAEQ,aAAqC;AACzC,WAAO;AAAA,MACH,gBAAgB;AAAA,MAChB,mBAAmB,KAAK,SAAS;AAAA,MACjC,GAAG,KAAK,SAAS;AAAA,IACrB;AAAA,EACJ;AAAA,EAEQ,gBAAgB,SAAuD;AAC3E,UAAM,WAA6B,CAAC;AAGpC,UAAM,iBAAiB,QAAQ;AAE/B,QAAI,CAAC,kBAAkB,CAAC,MAAM,QAAQ,cAAc,GAAG;AACnD,aAAO;AAAA,IACX;AAEA,eAAW,OAAO,gBAAgB;AAC9B,UAAI,UAAU;AAEd,UAAI,IAAI,SAAS,UAAU;AAEvB,kBAAU,IAAI;AAAA,MAClB,WAAW,IAAI,SAAS,UAAU,IAAI,SAAS,aAAa;AAExD,cAAM,aAAa,IAAI;AACvB,YAAI,MAAM,QAAQ,UAAU,GAAG;AAC3B,oBAAU,WACL,OAAO,CAAC,SAA2B,KAAK,SAAS,MAAM,EACvD,IAAI,CAAC,SAA0C,KAAK,QAAQ,EAAE,EAC9D,KAAK,EAAE;AAAA,QAChB,WAAW,OAAO,eAAe,UAAU;AACvC,oBAAU;AAAA,QACd;AAAA,MACJ;AAEA,UAAI,YAAY,IAAI,SAAS,YAAY,IAAI,SAAS,UAAU,IAAI,SAAS,cAAc;AACvF,iBAAS,KAAK;AAAA,UACV,MAAM,IAAI;AAAA,UACV;AAAA,QACJ,CAAC;AAAA,MACL;AAAA,IACJ;AAEA,WAAO;AAAA,EACX;AAAA,EAEQ,gBAAgB,QAA8C;AAClE,QAAI;AAEJ,YAAQ,QAAQ;AAAA,MACZ,KAAK;AAAA,MACL,KAAK;AACD,kBAAU;AACV;AAAA,MACJ,KAAK;AAAA,MACL,KAAK;AACD,kBAAU;AACV;AAAA,MACJ,KAAK;AACD,kBAAU;AACV;AAAA,MACJ,KAAK;AAAA,MACL,KAAK;AACD,kBAAU;AACV;AAAA,MACJ,KAAK;AACD,kBAAU;AACV;AAAA,MACJ;AACI,kBAAU;AAAA,IAClB;AAEA,WAAO,EAAE,SAAS,KAAK,OAAO;AAAA,EAClC;AAAA,EAEQ,WAAW,aAAqB,cAA4C;AAChF,WAAO;AAAA,MACH,aAAa;AAAA,QACT,OAAO;AAAA,QACP,SAAS;AAAA,QACT,WAAW;AAAA,QACX,YAAY;AAAA,MAChB;AAAA,MACA,cAAc;AAAA,QACV,OAAO;AAAA,QACP,MAAM;AAAA,QACN,WAAW;AAAA,MACf;AAAA,IACJ;AAAA,EACJ;AAAA;AAAA;AAAA;AAAA,EAKQ,aAAa,SAAgE;AACjF,QAAI,CAAC,QAAQ,SAAS,QAAQ,MAAM,WAAW,GAAG;AAC9C,aAAO;AAAA,IACX;AAEA,WAAO,QAAQ,MACV,OAAO,OAAK,EAAE,SAAS,UAAU,EACjC,IAAI,QAAM;AAAA,MACP,MAAM;AAAA,MACN,UAAU;AAAA,QACN,MAAM,EAAE;AAAA,QACR,aAAa,EAAE,eAAe;AAAA,QAC9B,YAAY,EAAE;AAAA,MAClB;AAAA,IACJ,EAAE;AAAA,EACV;AAAA;AAAA;AAAA;AAAA,EAKQ,kBAAkB,SAA4G;AAClI,UAAM,KAAK,QAAQ;AACnB,QAAI,CAAC,GAAI,QAAO;AAEhB,YAAQ,GAAG,MAAM;AAAA,MACb,KAAK;AACD,eAAO;AAAA,MACX,KAAK;AACD,eAAO;AAAA,MACX,KAAK;AACD,eAAO;AAAA,MACX,KAAK;AACD,eAAO,EAAE,MAAM,YAAY,UAAU,EAAE,MAAM,GAAG,SAAS,EAAE;AAAA,MAC/D;AACI,eAAO;AAAA,IACf;AAAA,EACJ;AAAA,EAEA,MAAM,WAAW,SAA6E;AAC1F,UAAM,WAAW,KAAK,gBAAgB,OAAO;AAC7C,UAAM,QAAQ,KAAK,aAAa,OAAO;AACvC,UAAM,aAAa,KAAK,kBAAkB,OAAO;AAEjD,UAAM,WAAW,MAAM,MAAM,GAAG,KAAK,SAAS,OAAO,gBAAgB;AAAA,MACjE,QAAQ;AAAA,MACR,SAAS,KAAK,WAAW;AAAA,MACzB,MAAM,KAAK,UAAU;AAAA,QACjB;AAAA,QACA,OAAO,KAAK;AAAA,QACZ,aAAa,QAAQ;AAAA,QACrB,WAAW,QAAQ;AAAA,QACnB,QAAQ;AAAA,QACR,QAAQ,KAAK,SAAS;AAAA,QACtB;AAAA,QACA;AAAA,MACJ,CAAC;AAAA,MACD,QAAQ,QAAQ;AAAA,IACpB,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AACd,YAAM,QAAQ,MAAM,SAAS,KAAK,EAAE,MAAM,OAAO,EAAE,OAAO,gBAAgB,EAAE;AAC5E,YAAM,IAAI,MAAM,sBAAsB,MAAM,SAAS,SAAS,UAAU,EAAE;AAAA,IAC9E;AAEA,UAAM,OAAO,MAAM,SAAS,KAAK;AAGjC,UAAM,UAAoC,CAAC;AAG3C,QAAI,KAAK,SAAS;AACd,cAAQ,KAAK;AAAA,QACT,MAAM;AAAA,QACN,MAAM,KAAK;AAAA,QACX,kBAAkB;AAAA,MACtB,CAAC;AAAA,IACL;AAGA,QAAI,KAAK,cAAc,KAAK,WAAW,SAAS,GAAG;AAC/C,iBAAW,MAAM,KAAK,YAAY;AAC9B,gBAAQ,KAAK;AAAA,UACT,MAAM;AAAA,UACN,YAAY,GAAG;AAAA,UACf,UAAU,GAAG,SAAS;AAAA,UACtB,OAAO,GAAG,SAAS;AAAA,UACnB,kBAAkB;AAAA,QACtB,CAAC;AAAA,MACL;AAAA,IACJ;AAEA,UAAM,WAA8B,CAAC;AAErC,WAAO;AAAA,MACH;AAAA,MACA,cAAc,KAAK,gBAAgB,KAAK,aAAa;AAAA,MACrD,OAAO,KAAK,WAAW,KAAK,MAAM,eAAe,KAAK,MAAM,iBAAiB;AAAA,MAC7E;AAAA,IACJ;AAAA,EACJ;AAAA,EAEA,MAAM,SAAS,SAA2E;AACtF,UAAM,WAAW,KAAK,gBAAgB,OAAO;AAC7C,UAAM,QAAQ,KAAK,aAAa,OAAO;AACvC,UAAM,aAAa,KAAK,kBAAkB,OAAO;AACjD,UAAM,OAAO;AAEb,UAAM,WAAW,MAAM,MAAM,GAAG,KAAK,SAAS,OAAO,gBAAgB;AAAA,MACjE,QAAQ;AAAA,MACR,SAAS,KAAK,WAAW;AAAA,MACzB,MAAM,KAAK,UAAU;AAAA,QACjB;AAAA,QACA,OAAO,KAAK;AAAA,QACZ,aAAa,QAAQ;AAAA,QACrB,WAAW,QAAQ;AAAA,QACnB,QAAQ;AAAA,QACR,QAAQ,KAAK,SAAS;AAAA,QACtB;AAAA,QACA;AAAA,MACJ,CAAC;AAAA,MACD,QAAQ,QAAQ;AAAA,IACpB,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AACd,YAAM,QAAQ,MAAM,SAAS,KAAK,EAAE,MAAM,OAAO,EAAE,OAAO,gBAAgB,EAAE;AAC5E,YAAM,IAAI,MAAM,sBAAsB,MAAM,SAAS,SAAS,UAAU,EAAE;AAAA,IAC9E;AAEA,UAAM,SAAS,SAAS,MAAM,UAAU;AACxC,QAAI,CAAC,QAAQ;AACT,YAAM,IAAI,MAAM,uBAAuB;AAAA,IAC3C;AAEA,UAAM,UAAU,IAAI,YAAY;AAChC,QAAI,SAAS;AACb,QAAI,cAAc;AAClB,QAAI,eAAe;AACnB,UAAM,aAAa;AACnB,QAAI,UAAU;AAEd,UAAM,SAAS,IAAI,eAA0C;AAAA,MACzD,MAAM,KAAK,YAAY;AACnB,YAAI;AACA,gBAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAE1C,cAAI,MAAM;AAEN,gBAAI,SAAS;AACT,yBAAW,QAAQ;AAAA,gBACf,MAAM;AAAA,gBACN,IAAI;AAAA,cACR,CAAC;AAAA,YACL;AACA,uBAAW,QAAQ;AAAA,cACf,MAAM;AAAA,cACN,cAAc,KAAK,gBAAgB,MAAM;AAAA,cACzC,OAAO,KAAK,WAAW,aAAa,YAAY;AAAA,YACpD,CAAC;AACD,uBAAW,MAAM;AACjB;AAAA,UACJ;AAEA,oBAAU,QAAQ,OAAO,OAAO,EAAE,QAAQ,KAAK,CAAC;AAChD,gBAAM,QAAQ,OAAO,MAAM,IAAI;AAC/B,mBAAS,MAAM,IAAI,KAAK;AAExB,qBAAW,QAAQ,OAAO;AACtB,gBAAI,KAAK,KAAK,MAAM,GAAI;AACxB,gBAAI,CAAC,KAAK,WAAW,QAAQ,EAAG;AAEhC,kBAAM,OAAO,KAAK,MAAM,CAAC;AACzB,gBAAI,SAAS,UAAU;AACnB,kBAAI,SAAS;AACT,2BAAW,QAAQ;AAAA,kBACf,MAAM;AAAA,kBACN,IAAI;AAAA,gBACR,CAAC;AAAA,cACL;AACA,yBAAW,QAAQ;AAAA,gBACf,MAAM;AAAA,gBACN,cAAc,KAAK,gBAAgB,MAAM;AAAA,gBACzC,OAAO,KAAK,WAAW,aAAa,YAAY;AAAA,cACpD,CAAC;AACD,yBAAW,MAAM;AACjB;AAAA,YACJ;AAEA,gBAAI;AACA,oBAAM,QAAQ,KAAK,MAAM,IAAI;AAG7B,kBAAI,MAAM,OAAO;AAEb,oBAAI,CAAC,SAAS;AACV,4BAAU;AACV,6BAAW,QAAQ;AAAA,oBACf,MAAM;AAAA,oBACN,IAAI;AAAA,kBACR,CAAC;AAAA,gBACL;AAEA,gCAAgB,KAAK,KAAK,MAAM,MAAM,SAAS,CAAC;AAChD,2BAAW,QAAQ;AAAA,kBACf,MAAM;AAAA,kBACN,IAAI;AAAA,kBACJ,OAAO,MAAM;AAAA,gBACjB,CAAC;AAAA,cACL;AAGA,kBAAI,MAAM,cAAc,MAAM,WAAW,SAAS,GAAG;AACjD,2BAAW,MAAM,MAAM,YAAY;AAE/B,6BAAW,QAAQ;AAAA,oBACf,MAAM;AAAA,oBACN,YAAY,GAAG;AAAA,oBACf,UAAU,GAAG,SAAS;AAAA,oBACtB,OAAO,GAAG,SAAS;AAAA,oBACnB,kBAAkB;AAAA,kBACtB,CAAC;AAAA,gBACL;AAAA,cACJ;AAEA,kBAAI,MAAM,eAAe;AACrB,oBAAI,SAAS;AACT,6BAAW,QAAQ;AAAA,oBACf,MAAM;AAAA,oBACN,IAAI;AAAA,kBACR,CAAC;AAAA,gBACL;AACA,2BAAW,QAAQ;AAAA,kBACf,MAAM;AAAA,kBACN,cAAc,KAAK,gBAAgB,MAAM,aAAa;AAAA,kBACtD,OAAO,KAAK,WAAW,aAAa,YAAY;AAAA,gBACpD,CAAC;AACD,2BAAW,MAAM;AACjB;AAAA,cACJ;AAAA,YACJ,QAAQ;AAAA,YAER;AAAA,UACJ;AAAA,QACJ,SAAS,OAAO;AACZ,qBAAW,MAAM,KAAK;AAAA,QAC1B;AAAA,MACJ;AAAA,MACA,SAAS;AACL,eAAO,OAAO;AAAA,MAClB;AAAA,IACJ,CAAC;AAED,WAAO;AAAA,MACH;AAAA,IACJ;AAAA,EACJ;AACJ;;;ACnYO,SAAS,cAAc,UAAmC,CAAC,GAAoB;AAClF,QAAM,UAAU,QAAQ,WAAW;AACnC,QAAM,SAAS,QAAQ,UAAU,QAAQ,IAAI;AAE7C,MAAI,CAAC,QAAQ;AACT,UAAM,IAAI,MAAM,sGAAsG;AAAA,EAC1H;AAEA,QAAM,cAAc,CAAC,SAAiB,WAAgC,CAAC,MAAM;AACzE,WAAO,IAAI,yBAAyB,SAAS;AAAA,MACzC;AAAA,MACA;AAAA,MACA,SAAS,QAAQ;AAAA,MACjB,GAAG;AAAA,IACP,CAAC;AAAA,EACL;AAEA,QAAM,WAAW,SAAU,SAAiB,UAAgC;AACxE,WAAO,YAAY,SAAS,QAAQ;AAAA,EACxC;AAEA,WAAS,OAAO;AAEhB,SAAO;AACX;AAeO,IAAM,UAA2B,SAAU,SAAiB,UAAgC;AAC/F,QAAM,SAAS,QAAQ,IAAI;AAC3B,MAAI,CAAC,QAAQ;AACT,UAAM,IAAI,MAAM,2GAA2G;AAAA,EAC/H;AACA,SAAO,IAAI,yBAAyB,SAAS;AAAA,IACzC;AAAA,IACA,SAAS;AAAA,IACT,GAAG;AAAA,EACP,CAAC;AACL;AAEA,QAAQ,OAAO;","names":[]}