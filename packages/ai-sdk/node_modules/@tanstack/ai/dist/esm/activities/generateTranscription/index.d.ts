import { TranscriptionAdapter } from './adapter.js';
import { TranscriptionResult } from '../../types.js';
/** The adapter kind this activity handles */
export declare const kind: "transcription";
/**
 * Extract provider options from a TranscriptionAdapter via ~types.
 */
export type TranscriptionProviderOptions<TAdapter> = TAdapter extends TranscriptionAdapter<any, any> ? TAdapter['~types']['providerOptions'] : object;
/**
 * Options for the transcription activity.
 * The model is extracted from the adapter's model property.
 *
 * @template TAdapter - The transcription adapter type
 */
export interface TranscriptionActivityOptions<TAdapter extends TranscriptionAdapter<string, object>> {
    /** The transcription adapter to use (must be created with a model) */
    adapter: TAdapter & {
        kind: typeof kind;
    };
    /** The audio data to transcribe - can be base64 string, File, Blob, or Buffer */
    audio: string | File | Blob | ArrayBuffer;
    /** The language of the audio in ISO-639-1 format (e.g., 'en') */
    language?: string;
    /** An optional prompt to guide the transcription */
    prompt?: string;
    /** The format of the transcription output */
    responseFormat?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt';
    /** Provider-specific options for transcription */
    modelOptions?: TranscriptionProviderOptions<TAdapter>;
}
/** Result type for the transcription activity */
export type TranscriptionActivityResult = Promise<TranscriptionResult>;
/**
 * Transcription activity - converts audio to text.
 *
 * Uses AI speech-to-text models to transcribe audio content.
 *
 * @example Transcribe an audio file
 * ```ts
 * import { generateTranscription } from '@tanstack/ai'
 * import { openaiTranscription } from '@tanstack/ai-openai'
 *
 * const result = await generateTranscription({
 *   adapter: openaiTranscription('whisper-1'),
 *   audio: audioFile, // File, Blob, or base64 string
 *   language: 'en'
 * })
 *
 * console.log(result.text)
 * ```
 *
 * @example With verbose output for timestamps
 * ```ts
 * const result = await generateTranscription({
 *   adapter: openaiTranscription('whisper-1'),
 *   audio: audioFile,
 *   responseFormat: 'verbose_json'
 * })
 *
 * result.segments?.forEach(segment => {
 *   console.log(`[${segment.start}s - ${segment.end}s]: ${segment.text}`)
 * })
 * ```
 */
export declare function generateTranscription<TAdapter extends TranscriptionAdapter<string, object>>(options: TranscriptionActivityOptions<TAdapter>): TranscriptionActivityResult;
/**
 * Create typed options for the generateTranscription() function without executing.
 */
export declare function createTranscriptionOptions<TAdapter extends TranscriptionAdapter<string, object>>(options: TranscriptionActivityOptions<TAdapter>): TranscriptionActivityOptions<TAdapter>;
export type { TranscriptionAdapter, TranscriptionAdapterConfig, AnyTranscriptionAdapter, } from './adapter.js';
export { BaseTranscriptionAdapter } from './adapter.js';
