import { aiEventClient } from "../../event-client.js";
import { streamToText } from "../../stream-to-response.js";
import { ToolCallManager, executeToolCalls } from "./tools/tool-calls.js";
import { convertSchemaToJsonSchema, isStandardSchema, parseWithStandardSchema } from "./tools/schema-converter.js";
import { maxIterations } from "./agent-loop-strategies.js";
const kind = "text";
function createChatOptions(options) {
  return options;
}
class TextEngine {
  constructor(config) {
    this.iterationCount = 0;
    this.lastFinishReason = null;
    this.streamStartTime = 0;
    this.totalChunkCount = 0;
    this.currentMessageId = null;
    this.accumulatedContent = "";
    this.doneChunk = null;
    this.shouldEmitStreamEnd = true;
    this.earlyTermination = false;
    this.toolPhase = "continue";
    this.cyclePhase = "processText";
    this.adapter = config.adapter;
    this.params = config.params;
    this.systemPrompts = config.params.systemPrompts || [];
    this.tools = config.params.tools || [];
    this.loopStrategy = config.params.agentLoopStrategy || maxIterations(5);
    this.toolCallManager = new ToolCallManager(this.tools);
    this.initialMessageCount = config.params.messages.length;
    this.messages = config.params.messages;
    this.requestId = this.createId("chat");
    this.streamId = this.createId("stream");
    this.effectiveRequest = config.params.abortController ? { signal: config.params.abortController.signal } : void 0;
    this.effectiveSignal = config.params.abortController?.signal;
  }
  /** Get the accumulated content after the chat loop completes */
  getAccumulatedContent() {
    return this.accumulatedContent;
  }
  /** Get the final messages array after the chat loop completes */
  getMessages() {
    return this.messages;
  }
  async *run() {
    this.beforeRun();
    try {
      const pendingPhase = yield* this.checkForPendingToolCalls();
      if (pendingPhase === "wait") {
        return;
      }
      do {
        if (this.earlyTermination || this.isAborted()) {
          return;
        }
        this.beginCycle();
        if (this.cyclePhase === "processText") {
          yield* this.streamModelResponse();
        } else {
          yield* this.processToolCalls();
        }
        this.endCycle();
      } while (this.shouldContinue());
    } finally {
      this.afterRun();
    }
  }
  beforeRun() {
    this.streamStartTime = Date.now();
    const {
      model,
      tools,
      temperature,
      topP,
      maxTokens,
      metadata,
      modelOptions,
      conversationId
    } = this.params;
    const options = {};
    if (temperature !== void 0) options.temperature = temperature;
    if (topP !== void 0) options.topP = topP;
    if (maxTokens !== void 0) options.maxTokens = maxTokens;
    if (metadata !== void 0) options.metadata = metadata;
    aiEventClient.emit("text:started", {
      requestId: this.requestId,
      streamId: this.streamId,
      model,
      provider: this.adapter.name,
      messageCount: this.initialMessageCount,
      hasTools: !!tools && tools.length > 0,
      streaming: true,
      timestamp: Date.now(),
      clientId: conversationId,
      toolNames: tools?.map((t) => t.name),
      options: Object.keys(options).length > 0 ? options : void 0,
      modelOptions
    });
    aiEventClient.emit("stream:started", {
      streamId: this.streamId,
      model,
      provider: this.adapter.name,
      timestamp: Date.now()
    });
  }
  afterRun() {
    if (!this.shouldEmitStreamEnd) {
      return;
    }
    const now = Date.now();
    aiEventClient.emit("text:completed", {
      requestId: this.requestId,
      streamId: this.streamId,
      model: this.params.model,
      content: this.accumulatedContent,
      messageId: this.currentMessageId || void 0,
      finishReason: this.lastFinishReason || void 0,
      usage: this.doneChunk?.usage,
      timestamp: now
    });
    aiEventClient.emit("stream:ended", {
      requestId: this.requestId,
      streamId: this.streamId,
      totalChunks: this.totalChunkCount,
      duration: now - this.streamStartTime,
      timestamp: now
    });
  }
  beginCycle() {
    if (this.cyclePhase === "processText") {
      this.beginIteration();
    }
  }
  endCycle() {
    if (this.cyclePhase === "processText") {
      this.cyclePhase = "executeToolCalls";
      return;
    }
    this.cyclePhase = "processText";
    this.iterationCount++;
  }
  beginIteration() {
    this.currentMessageId = this.createId("msg");
    this.accumulatedContent = "";
    this.doneChunk = null;
  }
  async *streamModelResponse() {
    const { temperature, topP, maxTokens, metadata, modelOptions } = this.params;
    const tools = this.params.tools;
    const toolsWithJsonSchemas = tools?.map((tool) => ({
      ...tool,
      inputSchema: tool.inputSchema ? convertSchemaToJsonSchema(tool.inputSchema) : void 0,
      outputSchema: tool.outputSchema ? convertSchemaToJsonSchema(tool.outputSchema) : void 0
    }));
    for await (const chunk of this.adapter.chatStream({
      model: this.params.model,
      messages: this.messages,
      tools: toolsWithJsonSchemas,
      temperature,
      topP,
      maxTokens,
      metadata,
      request: this.effectiveRequest,
      modelOptions,
      systemPrompts: this.systemPrompts
    })) {
      if (this.isAborted()) {
        break;
      }
      this.totalChunkCount++;
      yield chunk;
      this.handleStreamChunk(chunk);
      if (this.earlyTermination) {
        break;
      }
    }
  }
  handleStreamChunk(chunk) {
    switch (chunk.type) {
      case "content":
        this.handleContentChunk(chunk);
        break;
      case "tool_call":
        this.handleToolCallChunk(chunk);
        break;
      case "tool_result":
        this.handleToolResultChunk(chunk);
        break;
      case "done":
        this.handleDoneChunk(chunk);
        break;
      case "error":
        this.handleErrorChunk(chunk);
        break;
      case "thinking":
        this.handleThinkingChunk(chunk);
        break;
    }
  }
  handleContentChunk(chunk) {
    this.accumulatedContent = chunk.content;
    aiEventClient.emit("stream:chunk:content", {
      streamId: this.streamId,
      messageId: this.currentMessageId || void 0,
      content: chunk.content,
      delta: chunk.delta,
      timestamp: Date.now()
    });
  }
  handleToolCallChunk(chunk) {
    this.toolCallManager.addToolCallChunk(chunk);
    aiEventClient.emit("stream:chunk:tool-call", {
      streamId: this.streamId,
      messageId: this.currentMessageId || void 0,
      toolCallId: chunk.toolCall.id,
      toolName: chunk.toolCall.function.name,
      index: chunk.index,
      arguments: chunk.toolCall.function.arguments,
      timestamp: Date.now()
    });
  }
  handleToolResultChunk(chunk) {
    aiEventClient.emit("stream:chunk:tool-result", {
      streamId: this.streamId,
      messageId: this.currentMessageId || void 0,
      toolCallId: chunk.toolCallId,
      result: chunk.content,
      timestamp: Date.now()
    });
  }
  handleDoneChunk(chunk) {
    if (this.doneChunk?.finishReason === "tool_calls" && chunk.finishReason === "stop") {
      this.lastFinishReason = chunk.finishReason;
      aiEventClient.emit("stream:chunk:done", {
        streamId: this.streamId,
        messageId: this.currentMessageId || void 0,
        finishReason: chunk.finishReason,
        usage: chunk.usage,
        timestamp: Date.now()
      });
      if (chunk.usage) {
        aiEventClient.emit("usage:tokens", {
          requestId: this.requestId,
          streamId: this.streamId,
          messageId: this.currentMessageId || void 0,
          model: this.params.model,
          usage: chunk.usage,
          timestamp: Date.now()
        });
      }
      return;
    }
    this.doneChunk = chunk;
    this.lastFinishReason = chunk.finishReason;
    aiEventClient.emit("stream:chunk:done", {
      streamId: this.streamId,
      messageId: this.currentMessageId || void 0,
      finishReason: chunk.finishReason,
      usage: chunk.usage,
      timestamp: Date.now()
    });
    if (chunk.usage) {
      aiEventClient.emit("usage:tokens", {
        requestId: this.requestId,
        streamId: this.streamId,
        messageId: this.currentMessageId || void 0,
        model: this.params.model,
        usage: chunk.usage,
        timestamp: Date.now()
      });
    }
  }
  handleErrorChunk(chunk) {
    aiEventClient.emit("stream:chunk:error", {
      streamId: this.streamId,
      messageId: this.currentMessageId || void 0,
      error: chunk.error.message,
      timestamp: Date.now()
    });
    this.earlyTermination = true;
    this.shouldEmitStreamEnd = false;
  }
  handleThinkingChunk(chunk) {
    aiEventClient.emit("stream:chunk:thinking", {
      streamId: this.streamId,
      messageId: this.currentMessageId || void 0,
      content: chunk.content,
      delta: chunk.delta,
      timestamp: Date.now()
    });
  }
  async *checkForPendingToolCalls() {
    const pendingToolCalls = this.getPendingToolCallsFromMessages();
    if (pendingToolCalls.length === 0) {
      return "continue";
    }
    const doneChunk = this.createSyntheticDoneChunk();
    aiEventClient.emit("text:iteration", {
      requestId: this.requestId,
      streamId: this.streamId,
      iterationNumber: this.iterationCount + 1,
      messageCount: this.messages.length,
      toolCallCount: pendingToolCalls.length,
      timestamp: Date.now()
    });
    const { approvals, clientToolResults } = this.collectClientState();
    const executionResult = await executeToolCalls(
      pendingToolCalls,
      this.tools,
      approvals,
      clientToolResults
    );
    if (executionResult.needsApproval.length > 0 || executionResult.needsClientExecution.length > 0) {
      for (const chunk of this.emitApprovalRequests(
        executionResult.needsApproval,
        doneChunk
      )) {
        yield chunk;
      }
      for (const chunk of this.emitClientToolInputs(
        executionResult.needsClientExecution,
        doneChunk
      )) {
        yield chunk;
      }
      this.shouldEmitStreamEnd = false;
      return "wait";
    }
    const toolResultChunks = this.emitToolResults(
      executionResult.results,
      doneChunk
    );
    for (const chunk of toolResultChunks) {
      yield chunk;
    }
    return "continue";
  }
  async *processToolCalls() {
    if (!this.shouldExecuteToolPhase()) {
      this.setToolPhase("stop");
      return;
    }
    const toolCalls = this.toolCallManager.getToolCalls();
    const doneChunk = this.doneChunk;
    if (!doneChunk || toolCalls.length === 0) {
      this.setToolPhase("stop");
      return;
    }
    aiEventClient.emit("text:iteration", {
      requestId: this.requestId,
      streamId: this.streamId,
      iterationNumber: this.iterationCount + 1,
      messageCount: this.messages.length,
      toolCallCount: toolCalls.length,
      timestamp: Date.now()
    });
    this.addAssistantToolCallMessage(toolCalls);
    const { approvals, clientToolResults } = this.collectClientState();
    const executionResult = await executeToolCalls(
      toolCalls,
      this.tools,
      approvals,
      clientToolResults
    );
    if (executionResult.needsApproval.length > 0 || executionResult.needsClientExecution.length > 0) {
      for (const chunk of this.emitApprovalRequests(
        executionResult.needsApproval,
        doneChunk
      )) {
        yield chunk;
      }
      for (const chunk of this.emitClientToolInputs(
        executionResult.needsClientExecution,
        doneChunk
      )) {
        yield chunk;
      }
      this.setToolPhase("wait");
      return;
    }
    const toolResultChunks = this.emitToolResults(
      executionResult.results,
      doneChunk
    );
    for (const chunk of toolResultChunks) {
      yield chunk;
    }
    this.toolCallManager.clear();
    this.setToolPhase("continue");
  }
  shouldExecuteToolPhase() {
    return this.doneChunk?.finishReason === "tool_calls" && this.tools.length > 0 && this.toolCallManager.hasToolCalls();
  }
  addAssistantToolCallMessage(toolCalls) {
    this.messages = [
      ...this.messages,
      {
        role: "assistant",
        content: this.accumulatedContent || null,
        toolCalls
      }
    ];
  }
  collectClientState() {
    const approvals = /* @__PURE__ */ new Map();
    const clientToolResults = /* @__PURE__ */ new Map();
    for (const message of this.messages) {
      if (message.role === "assistant" && message.parts) {
        const parts = message.parts;
        for (const part of parts) {
          if (part.type === "tool-call" && part.state === "approval-responded" && part.approval) {
            approvals.set(part.approval.id, part.approval.approved);
          }
          if (part.type === "tool-call" && part.output !== void 0 && !part.approval) {
            clientToolResults.set(part.id, part.output);
          }
        }
      }
    }
    return { approvals, clientToolResults };
  }
  emitApprovalRequests(approvals, doneChunk) {
    const chunks = [];
    for (const approval of approvals) {
      aiEventClient.emit("stream:approval-requested", {
        streamId: this.streamId,
        messageId: this.currentMessageId || void 0,
        toolCallId: approval.toolCallId,
        toolName: approval.toolName,
        input: approval.input,
        approvalId: approval.approvalId,
        timestamp: Date.now()
      });
      chunks.push({
        type: "approval-requested",
        id: doneChunk.id,
        model: doneChunk.model,
        timestamp: Date.now(),
        toolCallId: approval.toolCallId,
        toolName: approval.toolName,
        input: approval.input,
        approval: {
          id: approval.approvalId,
          needsApproval: true
        }
      });
    }
    return chunks;
  }
  emitClientToolInputs(clientRequests, doneChunk) {
    const chunks = [];
    for (const clientTool of clientRequests) {
      aiEventClient.emit("stream:tool-input-available", {
        streamId: this.streamId,
        messageId: this.currentMessageId || void 0,
        toolCallId: clientTool.toolCallId,
        toolName: clientTool.toolName,
        input: clientTool.input,
        timestamp: Date.now()
      });
      chunks.push({
        type: "tool-input-available",
        id: doneChunk.id,
        model: doneChunk.model,
        timestamp: Date.now(),
        toolCallId: clientTool.toolCallId,
        toolName: clientTool.toolName,
        input: clientTool.input
      });
    }
    return chunks;
  }
  emitToolResults(results, doneChunk) {
    const chunks = [];
    for (const result of results) {
      aiEventClient.emit("tool:call-completed", {
        requestId: this.requestId,
        streamId: this.streamId,
        messageId: this.currentMessageId || void 0,
        toolCallId: result.toolCallId,
        toolName: result.toolName,
        result: result.result,
        duration: result.duration ?? 0,
        timestamp: Date.now()
      });
      const content = JSON.stringify(result.result);
      const chunk = {
        type: "tool_result",
        id: doneChunk.id,
        model: doneChunk.model,
        timestamp: Date.now(),
        toolCallId: result.toolCallId,
        content
      };
      chunks.push(chunk);
      this.messages = [
        ...this.messages,
        {
          role: "tool",
          content,
          toolCallId: result.toolCallId
        }
      ];
    }
    return chunks;
  }
  getPendingToolCallsFromMessages() {
    const completedToolIds = new Set(
      this.messages.filter((message) => message.role === "tool" && message.toolCallId).map((message) => message.toolCallId)
      // toolCallId exists due to filter
    );
    const pending = [];
    for (const message of this.messages) {
      if (message.role === "assistant" && message.toolCalls) {
        for (const toolCall of message.toolCalls) {
          if (!completedToolIds.has(toolCall.id)) {
            pending.push(toolCall);
          }
        }
      }
    }
    return pending;
  }
  createSyntheticDoneChunk() {
    return {
      type: "done",
      id: this.createId("pending"),
      model: this.params.model,
      timestamp: Date.now(),
      finishReason: "tool_calls"
    };
  }
  shouldContinue() {
    if (this.cyclePhase === "executeToolCalls") {
      return true;
    }
    return this.loopStrategy({
      iterationCount: this.iterationCount,
      messages: this.messages,
      finishReason: this.lastFinishReason
    }) && this.toolPhase === "continue";
  }
  isAborted() {
    return !!this.effectiveSignal?.aborted;
  }
  setToolPhase(phase) {
    this.toolPhase = phase;
    if (phase === "wait") {
      this.shouldEmitStreamEnd = false;
    }
  }
  createId(prefix) {
    return `${prefix}-${Date.now()}-${Math.random().toString(36).slice(2, 9)}`;
  }
}
function chat(options) {
  const { outputSchema, stream } = options;
  if (outputSchema) {
    return runAgenticStructuredOutput(
      options
    );
  }
  if (stream === false) {
    return runNonStreamingText(
      options
    );
  }
  return runStreamingText(
    options
  );
}
async function* runStreamingText(options) {
  const { adapter, ...textOptions } = options;
  const model = adapter.model;
  const engine = new TextEngine({
    adapter,
    params: { ...textOptions, model }
  });
  for await (const chunk of engine.run()) {
    yield chunk;
  }
}
function runNonStreamingText(options) {
  const stream = runStreamingText(
    options
  );
  return streamToText(stream);
}
async function runAgenticStructuredOutput(options) {
  const { adapter, outputSchema, ...textOptions } = options;
  const model = adapter.model;
  if (!outputSchema) {
    throw new Error("outputSchema is required for structured output");
  }
  const engine = new TextEngine({
    adapter,
    params: { ...textOptions, model }
  });
  for await (const _chunk of engine.run()) {
  }
  const finalMessages = engine.getMessages();
  const {
    tools: _tools,
    agentLoopStrategy: _als,
    ...structuredTextOptions
  } = textOptions;
  const jsonSchema = convertSchemaToJsonSchema(outputSchema);
  if (!jsonSchema) {
    throw new Error("Failed to convert output schema to JSON Schema");
  }
  const result = await adapter.structuredOutput({
    chatOptions: {
      ...structuredTextOptions,
      model,
      messages: finalMessages
    },
    outputSchema: jsonSchema
  });
  if (isStandardSchema(outputSchema)) {
    return parseWithStandardSchema(
      outputSchema,
      result.data
    );
  }
  return result.data;
}
export {
  chat,
  createChatOptions,
  kind
};
//# sourceMappingURL=index.js.map
